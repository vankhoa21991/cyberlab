{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a55978-5386-49b2-b842-d162f3a3e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from torchvision import models as Models\n",
    "from os import path as osp\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import sys\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils_yolo.augmentations import Yolov1Augmentation\n",
    "from utils_yolo.dataset import VOCDetection, detection_collate\n",
    "from utils_yolo.lr_scheduler import WarmUpMultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09163e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHANNEL_MEANS = (104, 117, 123)\n",
    "\n",
    "LEARNING_RATE: float = 0.001\n",
    "MOMENTUM: float = 0.9\n",
    "WEIGHT_DECAY: float = 0.0005\n",
    "STEP_LR_SIZES: List[int] = [200000, 400000]\n",
    "STEP_LR_GAMMA: float = 0.1\n",
    "WARM_UP_FACTOR: float = 0.1\n",
    "WARM_UP_NUM_ITERS: int = 1000\n",
    "\n",
    "NUM_STEPS_TO_SAVE: int = 100\n",
    "NUM_STEPS_TO_SNAPSHOT: int = 10000\n",
    "NUM_STEPS_TO_FINISH: int = 600000\n",
    "\n",
    "\n",
    "YOLOv1_PIC_SIZE = 448\n",
    "VOC_DATA_SET_ROOT = ''\n",
    "MODEL_SAVE_DIR = '/home/vankhoa/code/Object_detection/YOLOv1-Pytorch/model'\n",
    "GRID_NUM = 7\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 8\n",
    "save_step = 500\n",
    "backbone = \"resnet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58122241",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_data_set_root = \"/home/vankhoa/datasets/VOCtrainval_11-May-2012/VOCdevkit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b61a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VOCDetection(root=voc_data_set_root,\n",
    "                            image_sets=(('2007', 'trainval'), ('2012', 'trainval')),\n",
    "                            transform=Yolov1Augmentation(size=448, percent_coord=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a5ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = data.DataLoader(dataset,\n",
    "                            batch_size,\n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=detection_collate,\n",
    "                            pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63391a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_backbone(model_name: str):\n",
    "    r\"\"\"\n",
    "    get pre-trained base-network for yolo-v1,\n",
    "    children[:5] do not require grad\n",
    "    :param model_name: name of model\n",
    "    :return: pre-layer of pre-trained model without FC\n",
    "    \"\"\"\n",
    "\n",
    "    # when input shape is [, 3, 448, 448], output shape is:\n",
    "    features = list(Models.resnet50(True).children())[:-2]\n",
    "    for parameters in [feature.parameters() for i, feature in enumerate(features) if i <= 4]:\n",
    "        for parameter in parameters:\n",
    "            parameter.requires_grad = False\n",
    "    return nn.Sequential(*features), (2048, 14, 14)\n",
    "\n",
    "\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, backbone_name: str, grid_num=GRID_NUM, model_save_dir=MODEL_SAVE_DIR):\n",
    "        def get_tuple_multiplied(input_tuple: tuple):\n",
    "            res = 1.0\n",
    "            for i in input_tuple:\n",
    "                res *= i\n",
    "            return int(res)\n",
    "\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.grid_num = grid_num\n",
    "        self.backbone, feature_maps_shape = get_backbone(backbone_name)\n",
    "        self.model_save_name = '{}_{}'.format(self.__class__.__name__, backbone_name)\n",
    "        last_conv3x3_out_channel = 1024\n",
    "        self.last_conv3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=feature_maps_shape[0], out_channels=last_conv3x3_out_channel,\n",
    "                      kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(last_conv3x3_out_channel)\n",
    "        )\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(get_tuple_multiplied((last_conv3x3_out_channel, self.grid_num, self.grid_num)), 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(4096, int(self.grid_num * self.grid_num * 30)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.last_conv3x3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.cls(x)\n",
    "        x = torch.sigmoid(x)  # 归一化到0-1\n",
    "        x = x.view(-1, self.grid_num, self.grid_num, 30)\n",
    "        return x\n",
    "\n",
    "    def save_model(self, step=None, optimizer=None, lr_scheduler=None):\n",
    "        self.save_safely(self.state_dict(), self.model_save_dir, self.model_save_name + '.pkl')\n",
    "        print('*** model weights saved successfully at {}!'.format(\n",
    "            osp.join(self.model_save_dir, self.model_save_name + '.pkl')))\n",
    "        if optimizer and lr_scheduler and step is not None:\n",
    "            temp = {\n",
    "                'step': step,\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            self.save_safely(temp, self.model_save_dir, self.model_save_name + '_para.pkl')\n",
    "            print('*** auxiliary part saved successfully at {}!'.format(\n",
    "                osp.join(self.model_save_dir, self.model_save_name + '.pkl')))\n",
    "\n",
    "    def load_model(self, optimizer=None, lr_scheduler=None):\n",
    "        try:\n",
    "            saved_model = torch.load(osp.join(self.model_save_dir, self.model_save_name + '.pkl'),\n",
    "                                     map_location='cpu')\n",
    "            self.load_state_dict(saved_model)\n",
    "            print('*** loading model weight successfully!')\n",
    "        except Exception:\n",
    "            print('*** loading model weight fail!')\n",
    "\n",
    "        if optimizer and lr_scheduler is not None:\n",
    "            try:\n",
    "                temp = torch.load(osp.join(self.model_save_dir, self.model_save_name + '_para.pkl'), map_location='cpu')\n",
    "                lr_scheduler.load_state_dict(temp['lr_scheduler'])\n",
    "                step = temp['step']\n",
    "                print('*** loading optimizer&lr_scheduler&step successfully!')\n",
    "                return step\n",
    "            except Exception:\n",
    "                print('*** loading optimizer&lr_scheduler&step fail!')\n",
    "                return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def save_safely(file, dir_path, file_name):\n",
    "        r\"\"\"\n",
    "        save the file safely, if detect the file name conflict,\n",
    "        save the new file first and remove the old file\n",
    "        \"\"\"\n",
    "        if not osp.exists(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "            print('*** dir not exist, created one')\n",
    "        save_path = osp.join(dir_path, file_name)\n",
    "        if osp.exists(save_path):\n",
    "            temp_name = save_path + '.temp'\n",
    "            torch.save(file, temp_name)\n",
    "            os.remove(save_path)\n",
    "            os.rename(temp_name, save_path)\n",
    "            print('*** find the file conflict while saving, saved safely')\n",
    "        else:\n",
    "            torch.save(file, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9493df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    r\"\"\"Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n",
    "    Args:\n",
    "        box1: (tensor) bounding boxes, sized [N,4].\n",
    "        box2: (tensor) bounding boxes, sized [M,4].\n",
    "    Return:\n",
    "        (tensor) iou, sized [N,M].\n",
    "    \"\"\"\n",
    "    N = box1.size(0)\n",
    "    M = box2.size(0)\n",
    "    r'''\n",
    "    torch.max(input, other, out=None) → Tensor\n",
    "    Each element of the tensor input is compared with the corresponding element \n",
    "    of the tensor other and an element-wise maximum is taken.\n",
    "    '''\n",
    "    # left top\n",
    "    lt = torch.max(\n",
    "        box1[:, :2].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "        box2[:, :2].unsqueeze(0).expand(N, M, 2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "    )\n",
    "    # right bottom\n",
    "    rb = torch.min(\n",
    "        box1[:, 2:].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "        box2[:, 2:].unsqueeze(0).expand(N, M, 2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "    )\n",
    "\n",
    "    wh = rb - lt  # [N,M,2]\n",
    "    wh[wh < 0] = 0  # clip at 0\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])  # [N,]\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])  # [M,]\n",
    "    area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n",
    "    area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n",
    "\n",
    "    iou = inter / (area1 + area2 - inter)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401e9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Yolov1Loss(nn.Module):\n",
    "    def __init__(self, s=GRID_NUM, b=2, l_coord=5, l_noobj=0.5):\n",
    "        super(Yolov1Loss, self).__init__()\n",
    "        self.S = float(s)\n",
    "        self.B = int(b)\n",
    "        self.l_coord = l_coord\n",
    "        self.l_noobj = l_noobj\n",
    "\n",
    "    def forward(self, pred_tensor, target_tensor):\n",
    "        r\"\"\"\n",
    "        pred_tensor: (tensor) size(batchsize,S,S,Bx5+20=30) [x,y,w,h,c]\n",
    "        target_tensor: (tensor) size(batchsize,S,S,30)\n",
    "        \"\"\"\n",
    "        N = pred_tensor.size()[0]\n",
    "        # contain obj\n",
    "        coo_mask = target_tensor[:, :, :, 4] > 0 # 4x7x7\n",
    "        # no obj\n",
    "        noo_mask = target_tensor[:, :, :, 4] == 0 # 4x7x7\n",
    "        coo_mask = coo_mask.unsqueeze(-1).expand_as(target_tensor) # 4x7x7x30\n",
    "        noo_mask = noo_mask.unsqueeze(-1).expand_as(target_tensor) #4x7x7x30\n",
    "\n",
    "        # coo_pred：tensor[, 30]\n",
    "        coo_pred = pred_tensor[coo_mask].view(-1, 30) # 7x30\n",
    "        # box[x1,y1,w1,h1,c1], [x2,y2,w2,h2,c2]\n",
    "        box_pred = coo_pred[:, :10].contiguous().view(-1, 5) # 14x5\n",
    "        # class[...]\n",
    "        class_pred = coo_pred[:, 10:] # 7x20\n",
    "\n",
    "        coo_target = target_tensor[coo_mask].view(-1, 30) #7x30\n",
    "        box_target = coo_target[:, :10].contiguous().view(-1, 5) # 14x5\n",
    "        class_target = coo_target[:, 10:] # 7x20\n",
    "\n",
    "        # compute not contain obj loss\n",
    "        noo_pred = pred_tensor[noo_mask].view(-1, 30) # 189x30\n",
    "        noo_target = target_tensor[noo_mask].view(-1, 30) #189x30\n",
    "        # noo pred只需要计算 Obj1、2 的损失 size[,2]\n",
    "        noo_pred_mask = torch.ByteTensor(noo_pred.size()).to(DEVICE) # 189x30 matrix with 0\n",
    "        noo_pred_mask.zero_()\n",
    "        noo_pred_mask[:, 4] = 1 # confidence of first anchor\n",
    "        noo_pred_mask[:, 9] = 1 # confidence of second anchor\n",
    "        noo_pred_c = noo_pred[noo_pred_mask]\n",
    "        noo_target_c = noo_target[noo_pred_mask]\n",
    "        nooobj_loss = F.mse_loss(noo_pred_c, noo_target_c, reduction='sum')\n",
    "\n",
    "        # compute contain obj loss\n",
    "        coo_response_mask = torch.ByteTensor(box_target.size()).to(DEVICE)\n",
    "        coo_response_mask.zero_() # 14x5\n",
    "        coo_not_response_mask = torch.ByteTensor(box_target.size()).to(DEVICE)\n",
    "        coo_not_response_mask.zero_() # 14x5\n",
    "        box_target_iou = torch.zeros(box_target.size()).to(DEVICE) # 14x5\n",
    "        # get anchor with biggest iou, use that anchor to compute loss\n",
    "        for i in range(0, box_target.size()[0], 2):  # choose the best iou box\n",
    "            box1 = box_pred[i:i + 2]\n",
    "            box1_xyxy = torch.FloatTensor(box1.size())\n",
    "            # (x,y,w,h)\n",
    "            box1_xyxy[:, :2] = box1[:, :2] / self.S - 0.5 * box1[:, 2:4]\n",
    "            box1_xyxy[:, 2:4] = box1[:, :2] / self.S + 0.5 * box1[:, 2:4]\n",
    "            box2 = box_target[i].view(-1, 5)\n",
    "            box2_xyxy = torch.FloatTensor(box2.size())\n",
    "            box2_xyxy[:, :2] = box2[:, :2] / self.S - 0.5 * box2[:, 2:4]\n",
    "            box2_xyxy[:, 2:4] = box2[:, :2] / self.S + 0.5 * box2[:, 2:4]\n",
    "            # iou(pred_box[2,], target_box[2,])\n",
    "            iou = compute_iou(box1_xyxy[:, :4], box2_xyxy[:, :4])\n",
    "            # target匹配到的box\n",
    "            max_iou, max_index = iou.max(0)\n",
    "            # print(f'max_iou:{max_iou}, max_index:{max_index}')\n",
    "            max_index = max_index.to(DEVICE)\n",
    "\n",
    "            coo_response_mask[i + max_index] = 1\n",
    "            coo_not_response_mask[i + 1 - max_index] = 1\n",
    "            #####\n",
    "            # we want the confidence score to equal the\n",
    "            # intersection over union (IOU) between the predicted box\n",
    "            # and the ground truth\n",
    "            #####\n",
    "            box_target_iou[i + max_index, torch.LongTensor([4]).to(DEVICE)] = max_iou.to(DEVICE)\n",
    "\n",
    "        box_target_iou = box_target_iou.to(DEVICE)\n",
    "        # 1.response loss\n",
    "        box_pred_response = box_pred[coo_response_mask].view(-1, 5)\n",
    "        box_target_response_iou = box_target_iou[coo_response_mask].view(-1, 5)\n",
    "        box_target_response = box_target[coo_response_mask].view(-1, 5)\n",
    "        contain_loss = F.mse_loss(box_pred_response[:, 4], box_target_response_iou[:, 4], reduction='sum') # C confidence\n",
    "\n",
    "        loc_loss = F.mse_loss(box_pred_response[:, :2], box_target_response[:, :2], reduction='sum') + F.mse_loss(\n",
    "            torch.sqrt(box_pred_response[:, 2:4]), torch.sqrt(box_target_response[:, 2:4]), reduction='sum') # XY, WH\n",
    "\n",
    "        # 2.not response loss\n",
    "        box_pred_not_response = box_pred[coo_not_response_mask].view(-1, 5)\n",
    "        box_target_not_response = box_target[coo_not_response_mask].view(-1, 5)\n",
    "        box_target_not_response[:, 4] = 0\n",
    "        # not_contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response[:,4],size_average=False)\n",
    "\n",
    "        # I believe this bug is simply a typo\n",
    "        not_contain_loss = F.mse_loss(box_pred_not_response[:, 4], box_target_not_response[:, 4], reduction='sum')  # I don't think this should be counted\n",
    "\n",
    "        # 3.class loss\n",
    "        class_loss = F.mse_loss(class_pred, class_target, reduction='sum')\n",
    "\n",
    "        return (self.l_coord * loc_loss + 2 * contain_loss\n",
    "                + not_contain_loss\n",
    "                + self.l_noobj * nooobj_loss\n",
    "                + class_loss) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115ba945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** loading model weight fail!\n",
      "*** loading optimizer&lr_scheduler&step fail!\n"
     ]
    }
   ],
   "source": [
    "model = Yolov1(backbone_name=backbone)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                        lr=LEARNING_RATE,\n",
    "                        momentum=MOMENTUM,\n",
    "                        weight_decay=WEIGHT_DECAY)\n",
    "scheduler = WarmUpMultiStepLR(optimizer,\n",
    "                                milestones=STEP_LR_SIZES,\n",
    "                                gamma=STEP_LR_GAMMA,\n",
    "                                warm_up_factor=WARM_UP_FACTOR,\n",
    "                                warm_up_iters=WARM_UP_NUM_ITERS)\n",
    "step = model.load_model(optimizer=optimizer, lr_scheduler=scheduler)\n",
    "criterion = Yolov1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d79663d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:1 | loss:22.84800339 | time:1.2162\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 7.79 GiB total capacity; 3.57 GiB already allocated; 410.44 MiB free; 3.67 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-44b49e4de0ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cuda101/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cuda101/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 7.79 GiB total capacity; 3.57 GiB already allocated; 410.44 MiB free; 3.67 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model.to(DEVICE)\n",
    "model.train()\n",
    "while step < NUM_STEPS_TO_FINISH:\n",
    "    t1 = time.perf_counter()\n",
    "    for _, (imgs, gt_boxes, gt_labels, gt_outs) in enumerate(dataloader):\n",
    "        step += 1\n",
    "        scheduler.step()\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        gt_outs = gt_outs.to(DEVICE)\n",
    "        model_outs = model(imgs)\n",
    "        loss = criterion(model_outs, gt_outs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t2 = time.perf_counter()\n",
    "        print('step:{} | loss:{:.8f} | time:{:.4f}'.format(step, loss.item(), t2 - t1))\n",
    "        t1 = time.perf_counter()\n",
    "        if step != 0 and step % save_step == 0:\n",
    "            model.save_model(step, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377d9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35cdea0ed224c2574ca597c0b1cda92a3faa5a5a5b5667b5b90cbe9151ca726f"
  },
  "kernelspec": {
   "display_name": "py36d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
