{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sn6bc1lLuU5I"
      },
      "source": [
        "# Sequence to sequence learning for performing number addition\n",
        "\n",
        "**Author:** [Smerity](https://twitter.com/Smerity) and others<br>\n",
        "**Date created:** 2015/08/17<br>\n",
        "**Last modified:** 2020/04/17<br>\n",
        "**Description:** A model that learns to add strings of numbers, e.g. \"535+61\" -> \"596\"."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WYEI30Q4uU5L"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we train a model to learn to add two numbers, provided as strings.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- Input: \"535+61\"\n",
        "- Output: \"596\"\n",
        "\n",
        "Input may optionally be reversed, which was shown to increase performance in many tasks\n",
        " in: [Learning to Execute](http://arxiv.org/abs/1410.4615) and\n",
        "[Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).\n",
        "\n",
        "Theoretically, sequence order inversion introduces shorter term dependencies between\n",
        " source and target for this problem.\n",
        "\n",
        "**Results:**\n",
        "\n",
        "For two digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
        "\n",
        "Three digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
        "\n",
        "Four digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
        "\n",
        "Five digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zgMBB3KhuU5M"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8owtqDWXuU5M"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_xDAsRytuU5N"
      },
      "source": [
        "## Generate the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X2xvbLwuuU5N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total questions: 50000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class CharacterTable:\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = \"0123456789+ \"\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(\n",
        "        \"\".join(\n",
        "            np.random.choice(list(\"0123456789\"))\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))\n",
        "        )\n",
        "    )\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print(\"Total questions:\", len(questions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F5vf35k7uU5O"
      },
      "source": [
        "## Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GUmTANdQuU5O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorization...\n",
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ]
        }
      ],
      "source": [
        "print(\"Vectorization...\")\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5eQ1mVZuU5P"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h87cI6xauU5P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4, 12)             1548      \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print(\"Build model...\")\n",
        "num_layers = 1  # Try to add more LSTM layers!\n",
        "\n",
        "model = keras.Sequential()\n",
        "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
        "# As the decoder RNN's input, repeatedly provide with the last output of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(num_layers):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(layers.LSTM(128, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yzLdCBVOuU5P"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nygLct7SuU5P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.7397 - accuracy: 0.3625 - val_loss: 1.5305 - val_accuracy: 0.4261\n",
            "Q 835+646 T 1481 ☒ 1414\n",
            "Q 254+95  T 349  ☒ 559 \n",
            "Q 5+983   T 988  ☒ 990 \n",
            "Q 744+431 T 1175 ☒ 1214\n",
            "Q 538+41  T 579  ☒ 577 \n",
            "Q 329+76  T 405  ☒ 399 \n",
            "Q 813+256 T 1069 ☒ 1010\n",
            "Q 64+282  T 346  ☒ 377 \n",
            "Q 26+940  T 966  ☒ 904 \n",
            "Q 74+173  T 247  ☒ 277 \n",
            "\n",
            "Iteration 2\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3248 - accuracy: 0.5034 - val_loss: 1.1778 - val_accuracy: 0.5602\n",
            "Q 42+988  T 1030 ☒ 1037\n",
            "Q 491+590 T 1081 ☒ 100 \n",
            "Q 706+23  T 729  ☒ 737 \n",
            "Q 82+36   T 118  ☒ 110 \n",
            "Q 604+12  T 616  ☒ 620 \n",
            "Q 264+2   T 266  ☒ 268 \n",
            "Q 273+485 T 758  ☒ 867 \n",
            "Q 144+53  T 197  ☒ 183 \n",
            "Q 139+0   T 139  ☒ 137 \n",
            "Q 16+2    T 18   ☒ 35  \n",
            "\n",
            "Iteration 3\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 1.0532 - accuracy: 0.6088 - val_loss: 0.9573 - val_accuracy: 0.6475\n",
            "Q 1+387   T 388  ☒ 395 \n",
            "Q 491+556 T 1047 ☒ 1089\n",
            "Q 50+552  T 602  ☒ 698 \n",
            "Q 268+84  T 352  ☒ 355 \n",
            "Q 72+940  T 1012 ☒ 1004\n",
            "Q 990+8   T 998  ☒ 990 \n",
            "Q 549+5   T 554  ☒ 552 \n",
            "Q 532+970 T 1502 ☒ 1519\n",
            "Q 32+19   T 51   ☒ 40  \n",
            "Q 99+399  T 498  ☒ 468 \n",
            "\n",
            "Iteration 4\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.8952 - accuracy: 0.6700 - val_loss: 0.8516 - val_accuracy: 0.6762\n",
            "Q 426+3   T 429  ☒ 427 \n",
            "Q 418+28  T 446  ☒ 440 \n",
            "Q 191+30  T 221  ☒ 223 \n",
            "Q 70+814  T 884  ☒ 871 \n",
            "Q 108+587 T 695  ☒ 690 \n",
            "Q 978+64  T 1042 ☒ 1030\n",
            "Q 442+11  T 453  ☒ 457 \n",
            "Q 740+266 T 1006 ☒ 1000\n",
            "Q 65+262  T 327  ☒ 329 \n",
            "Q 74+560  T 634  ☒ 627 \n",
            "\n",
            "Iteration 5\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.7858 - accuracy: 0.7128 - val_loss: 0.7417 - val_accuracy: 0.7278\n",
            "Q 333+0   T 333  ☑ 333 \n",
            "Q 235+1   T 236  ☑ 236 \n",
            "Q 478+61  T 539  ☒ 531 \n",
            "Q 456+211 T 667  ☒ 677 \n",
            "Q 81+866  T 947  ☒ 945 \n",
            "Q 304+278 T 582  ☒ 588 \n",
            "Q 893+80  T 973  ☒ 972 \n",
            "Q 581+178 T 759  ☒ 757 \n",
            "Q 23+989  T 1012 ☒ 1018\n",
            "Q 81+951  T 1032 ☒ 1021\n",
            "\n",
            "Iteration 6\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.6966 - accuracy: 0.7465 - val_loss: 0.6514 - val_accuracy: 0.7575\n",
            "Q 19+114  T 133  ☒ 139 \n",
            "Q 948+846 T 1794 ☒ 1797\n",
            "Q 784+653 T 1437 ☒ 1435\n",
            "Q 33+571  T 604  ☑ 604 \n",
            "Q 797+84  T 881  ☒ 872 \n",
            "Q 33+804  T 837  ☒ 846 \n",
            "Q 417+754 T 1171 ☒ 1177\n",
            "Q 521+20  T 541  ☒ 543 \n",
            "Q 30+924  T 954  ☒ 952 \n",
            "Q 441+76  T 517  ☒ 519 \n",
            "\n",
            "Iteration 7\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.5096 - accuracy: 0.8128 - val_loss: 0.3941 - val_accuracy: 0.8609\n",
            "Q 703+297 T 1000 ☒ 900 \n",
            "Q 69+329  T 398  ☒ 397 \n",
            "Q 96+615  T 711  ☑ 711 \n",
            "Q 3+258   T 261  ☑ 261 \n",
            "Q 577+240 T 817  ☒ 818 \n",
            "Q 947+996 T 1943 ☒ 1944\n",
            "Q 314+721 T 1035 ☑ 1035\n",
            "Q 30+69   T 99   ☒ 90  \n",
            "Q 466+727 T 1193 ☒ 1192\n",
            "Q 4+181   T 185  ☑ 185 \n",
            "\n",
            "Iteration 8\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.2670 - accuracy: 0.9192 - val_loss: 0.2521 - val_accuracy: 0.9150\n",
            "Q 628+2   T 630  ☑ 630 \n",
            "Q 92+333  T 425  ☑ 425 \n",
            "Q 45+668  T 713  ☑ 713 \n",
            "Q 61+71   T 132  ☑ 132 \n",
            "Q 264+16  T 280  ☑ 280 \n",
            "Q 143+466 T 609  ☑ 609 \n",
            "Q 97+56   T 153  ☑ 153 \n",
            "Q 944+221 T 1165 ☑ 1165\n",
            "Q 840+830 T 1670 ☒ 1660\n",
            "Q 943+614 T 1557 ☑ 1557\n",
            "\n",
            "Iteration 9\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.1398 - accuracy: 0.9659 - val_loss: 0.1099 - val_accuracy: 0.9717\n",
            "Q 2+960   T 962  ☒ 963 \n",
            "Q 4+115   T 119  ☑ 119 \n",
            "Q 655+960 T 1615 ☑ 1615\n",
            "Q 88+403  T 491  ☑ 491 \n",
            "Q 94+22   T 116  ☑ 116 \n",
            "Q 4+323   T 327  ☑ 327 \n",
            "Q 790+134 T 924  ☑ 924 \n",
            "Q 30+705  T 735  ☑ 735 \n",
            "Q 532+970 T 1502 ☑ 1502\n",
            "Q 858+8   T 866  ☑ 866 \n",
            "\n",
            "Iteration 10\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0897 - accuracy: 0.9779 - val_loss: 0.0762 - val_accuracy: 0.9796\n",
            "Q 845+5   T 850  ☑ 850 \n",
            "Q 624+427 T 1051 ☑ 1051\n",
            "Q 410+18  T 428  ☑ 428 \n",
            "Q 246+352 T 598  ☑ 598 \n",
            "Q 92+850  T 942  ☑ 942 \n",
            "Q 23+250  T 273  ☑ 273 \n",
            "Q 240+34  T 274  ☑ 274 \n",
            "Q 449+351 T 800  ☑ 800 \n",
            "Q 31+526  T 557  ☑ 557 \n",
            "Q 22+375  T 397  ☑ 397 \n",
            "\n",
            "Iteration 11\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0622 - accuracy: 0.9849 - val_loss: 0.0427 - val_accuracy: 0.9905\n",
            "Q 226+3   T 229  ☑ 229 \n",
            "Q 875+513 T 1388 ☒ 1288\n",
            "Q 884+5   T 889  ☑ 889 \n",
            "Q 484+803 T 1287 ☑ 1287\n",
            "Q 26+104  T 130  ☑ 130 \n",
            "Q 472+21  T 493  ☑ 493 \n",
            "Q 93+419  T 512  ☑ 512 \n",
            "Q 26+619  T 645  ☑ 645 \n",
            "Q 744+498 T 1242 ☑ 1242\n",
            "Q 374+278 T 652  ☑ 652 \n",
            "\n",
            "Iteration 12\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0477 - accuracy: 0.9879 - val_loss: 0.0560 - val_accuracy: 0.9825\n",
            "Q 537+584 T 1121 ☑ 1121\n",
            "Q 436+669 T 1105 ☑ 1105\n",
            "Q 73+478  T 551  ☑ 551 \n",
            "Q 336+244 T 580  ☑ 580 \n",
            "Q 112+44  T 156  ☑ 156 \n",
            "Q 833+10  T 843  ☑ 843 \n",
            "Q 215+60  T 275  ☑ 275 \n",
            "Q 885+18  T 903  ☑ 903 \n",
            "Q 947+3   T 950  ☑ 950 \n",
            "Q 756+403 T 1159 ☑ 1159\n",
            "\n",
            "Iteration 13\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0369 - accuracy: 0.9904 - val_loss: 0.0305 - val_accuracy: 0.9923\n",
            "Q 97+34   T 131  ☑ 131 \n",
            "Q 901+487 T 1388 ☑ 1388\n",
            "Q 232+27  T 259  ☑ 259 \n",
            "Q 255+474 T 729  ☑ 729 \n",
            "Q 24+63   T 87   ☑ 87  \n",
            "Q 358+537 T 895  ☑ 895 \n",
            "Q 93+494  T 587  ☑ 587 \n",
            "Q 391+53  T 444  ☑ 444 \n",
            "Q 199+62  T 261  ☑ 261 \n",
            "Q 87+657  T 744  ☑ 744 \n",
            "\n",
            "Iteration 14\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0434 - accuracy: 0.9881 - val_loss: 0.0264 - val_accuracy: 0.9931\n",
            "Q 72+299  T 371  ☒ 381 \n",
            "Q 489+489 T 978  ☑ 978 \n",
            "Q 46+891  T 937  ☑ 937 \n",
            "Q 461+7   T 468  ☑ 468 \n",
            "Q 65+34   T 99   ☑ 99  \n",
            "Q 60+34   T 94   ☑ 94  \n",
            "Q 718+18  T 736  ☑ 736 \n",
            "Q 104+43  T 147  ☑ 147 \n",
            "Q 38+579  T 617  ☑ 617 \n",
            "Q 819+29  T 848  ☑ 848 \n",
            "\n",
            "Iteration 15\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0214 - accuracy: 0.9950 - val_loss: 0.0722 - val_accuracy: 0.9765\n",
            "Q 804+50  T 854  ☑ 854 \n",
            "Q 80+633  T 713  ☑ 713 \n",
            "Q 22+50   T 72   ☑ 72  \n",
            "Q 521+739 T 1260 ☑ 1260\n",
            "Q 1+297   T 298  ☒ 398 \n",
            "Q 90+19   T 109  ☒ 119 \n",
            "Q 22+375  T 397  ☑ 397 \n",
            "Q 93+201  T 294  ☑ 294 \n",
            "Q 88+453  T 541  ☑ 541 \n",
            "Q 557+281 T 838  ☑ 838 \n",
            "\n",
            "Iteration 16\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0365 - accuracy: 0.9898 - val_loss: 0.0203 - val_accuracy: 0.9948\n",
            "Q 91+220  T 311  ☑ 311 \n",
            "Q 56+604  T 660  ☑ 660 \n",
            "Q 267+69  T 336  ☑ 336 \n",
            "Q 569+49  T 618  ☑ 618 \n",
            "Q 94+964  T 1058 ☑ 1058\n",
            "Q 806+961 T 1767 ☑ 1767\n",
            "Q 9+704   T 713  ☑ 713 \n",
            "Q 5+519   T 524  ☑ 524 \n",
            "Q 170+432 T 602  ☑ 602 \n",
            "Q 83+174  T 257  ☑ 257 \n",
            "\n",
            "Iteration 17\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0306 - accuracy: 0.9919 - val_loss: 0.0602 - val_accuracy: 0.9812\n",
            "Q 502+27  T 529  ☑ 529 \n",
            "Q 92+971  T 1063 ☑ 1063\n",
            "Q 274+163 T 437  ☑ 437 \n",
            "Q 3+260   T 263  ☑ 263 \n",
            "Q 292+795 T 1087 ☑ 1087\n",
            "Q 2+598   T 600  ☑ 600 \n",
            "Q 521+337 T 858  ☑ 858 \n",
            "Q 353+533 T 886  ☑ 886 \n",
            "Q 88+665  T 753  ☑ 753 \n",
            "Q 1+156   T 157  ☑ 157 \n",
            "\n",
            "Iteration 18\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0231 - accuracy: 0.9937 - val_loss: 0.0156 - val_accuracy: 0.9966\n",
            "Q 456+71  T 527  ☑ 527 \n",
            "Q 713+357 T 1070 ☑ 1070\n",
            "Q 9+704   T 713  ☑ 713 \n",
            "Q 86+745  T 831  ☑ 831 \n",
            "Q 5+18    T 23   ☑ 23  \n",
            "Q 28+267  T 295  ☑ 295 \n",
            "Q 319+8   T 327  ☑ 327 \n",
            "Q 60+863  T 923  ☑ 923 \n",
            "Q 277+361 T 638  ☑ 638 \n",
            "Q 24+69   T 93   ☑ 93  \n",
            "\n",
            "Iteration 19\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0241 - accuracy: 0.9935 - val_loss: 0.0297 - val_accuracy: 0.9912\n",
            "Q 851+1   T 852  ☑ 852 \n",
            "Q 411+957 T 1368 ☑ 1368\n",
            "Q 666+82  T 748  ☑ 748 \n",
            "Q 985+0   T 985  ☑ 985 \n",
            "Q 969+9   T 978  ☑ 978 \n",
            "Q 686+928 T 1614 ☑ 1614\n",
            "Q 415+91  T 506  ☑ 506 \n",
            "Q 9+412   T 421  ☑ 421 \n",
            "Q 418+28  T 446  ☑ 446 \n",
            "Q 829+38  T 867  ☑ 867 \n",
            "\n",
            "Iteration 20\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0137 - accuracy: 0.9967 - val_loss: 0.0702 - val_accuracy: 0.9790\n",
            "Q 17+955  T 972  ☑ 972 \n",
            "Q 428+570 T 998  ☑ 998 \n",
            "Q 453+370 T 823  ☑ 823 \n",
            "Q 545+17  T 562  ☑ 562 \n",
            "Q 80+780  T 860  ☑ 860 \n",
            "Q 93+725  T 818  ☑ 818 \n",
            "Q 581+55  T 636  ☑ 636 \n",
            "Q 216+60  T 276  ☑ 276 \n",
            "Q 96+960  T 1056 ☑ 1056\n",
            "Q 1+642   T 643  ☑ 643 \n",
            "\n",
            "Iteration 21\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0202 - accuracy: 0.9945 - val_loss: 0.0101 - val_accuracy: 0.9973\n",
            "Q 521+20  T 541  ☑ 541 \n",
            "Q 2+42    T 44   ☑ 44  \n",
            "Q 868+934 T 1802 ☑ 1802\n",
            "Q 9+129   T 138  ☑ 138 \n",
            "Q 178+655 T 833  ☑ 833 \n",
            "Q 51+97   T 148  ☑ 148 \n",
            "Q 492+80  T 572  ☑ 572 \n",
            "Q 835+646 T 1481 ☑ 1481\n",
            "Q 838+841 T 1679 ☑ 1679\n",
            "Q 45+547  T 592  ☑ 592 \n",
            "\n",
            "Iteration 22\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0157 - accuracy: 0.9957 - val_loss: 0.0332 - val_accuracy: 0.9899\n",
            "Q 73+65   T 138  ☑ 138 \n",
            "Q 289+92  T 381  ☑ 381 \n",
            "Q 148+72  T 220  ☑ 220 \n",
            "Q 887+401 T 1288 ☑ 1288\n",
            "Q 806+675 T 1481 ☑ 1481\n",
            "Q 170+6   T 176  ☑ 176 \n",
            "Q 2+598   T 600  ☑ 600 \n",
            "Q 490+68  T 558  ☑ 558 \n",
            "Q 587+860 T 1447 ☑ 1447\n",
            "Q 208+261 T 469  ☑ 469 \n",
            "\n",
            "Iteration 23\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0149 - accuracy: 0.9962 - val_loss: 0.0198 - val_accuracy: 0.9934\n",
            "Q 704+529 T 1233 ☒ 1333\n",
            "Q 65+34   T 99   ☑ 99  \n",
            "Q 80+99   T 179  ☑ 179 \n",
            "Q 2+255   T 257  ☑ 257 \n",
            "Q 31+527  T 558  ☑ 558 \n",
            "Q 771+87  T 858  ☑ 858 \n",
            "Q 825+44  T 869  ☑ 869 \n",
            "Q 903+893 T 1796 ☑ 1796\n",
            "Q 357+20  T 377  ☑ 377 \n",
            "Q 792+568 T 1360 ☑ 1360\n",
            "\n",
            "Iteration 24\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0235 - accuracy: 0.9931 - val_loss: 0.0370 - val_accuracy: 0.9872\n",
            "Q 95+481  T 576  ☑ 576 \n",
            "Q 128+10  T 138  ☑ 138 \n",
            "Q 463+85  T 548  ☑ 548 \n",
            "Q 289+375 T 664  ☑ 664 \n",
            "Q 61+71   T 132  ☑ 132 \n",
            "Q 221+1   T 222  ☑ 222 \n",
            "Q 7+55    T 62   ☑ 62  \n",
            "Q 522+43  T 565  ☑ 565 \n",
            "Q 583+554 T 1137 ☑ 1137\n",
            "Q 70+423  T 493  ☑ 493 \n",
            "\n",
            "Iteration 25\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0081 - accuracy: 0.9982 - val_loss: 0.0037 - val_accuracy: 0.9992\n",
            "Q 148+257 T 405  ☑ 405 \n",
            "Q 302+6   T 308  ☑ 308 \n",
            "Q 456+559 T 1015 ☑ 1015\n",
            "Q 77+221  T 298  ☑ 298 \n",
            "Q 92+850  T 942  ☑ 942 \n",
            "Q 82+222  T 304  ☑ 304 \n",
            "Q 593+2   T 595  ☑ 595 \n",
            "Q 319+537 T 856  ☑ 856 \n",
            "Q 97+768  T 865  ☑ 865 \n",
            "Q 328+0   T 328  ☑ 328 \n",
            "\n",
            "Iteration 26\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0197 - accuracy: 0.9945 - val_loss: 0.0057 - val_accuracy: 0.9987\n",
            "Q 438+13  T 451  ☑ 451 \n",
            "Q 938+14  T 952  ☑ 952 \n",
            "Q 20+778  T 798  ☑ 798 \n",
            "Q 835+53  T 888  ☑ 888 \n",
            "Q 68+245  T 313  ☑ 313 \n",
            "Q 39+687  T 726  ☑ 726 \n",
            "Q 690+75  T 765  ☑ 765 \n",
            "Q 78+974  T 1052 ☑ 1052\n",
            "Q 93+433  T 526  ☑ 526 \n",
            "Q 554+51  T 605  ☑ 605 \n",
            "\n",
            "Iteration 27\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0595 - val_accuracy: 0.9791\n",
            "Q 930+89  T 1019 ☑ 1019\n",
            "Q 5+460   T 465  ☑ 465 \n",
            "Q 3+994   T 997  ☒ 996 \n",
            "Q 694+760 T 1454 ☒ 1464\n",
            "Q 629+7   T 636  ☑ 636 \n",
            "Q 65+879  T 944  ☑ 944 \n",
            "Q 67+796  T 863  ☑ 863 \n",
            "Q 70+420  T 490  ☒ 400 \n",
            "Q 338+361 T 699  ☑ 699 \n",
            "Q 674+14  T 688  ☑ 688 \n",
            "\n",
            "Iteration 28\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.0178 - val_accuracy: 0.9945\n",
            "Q 971+223 T 1194 ☑ 1194\n",
            "Q 92+434  T 526  ☑ 526 \n",
            "Q 77+913  T 990  ☑ 990 \n",
            "Q 959+276 T 1235 ☑ 1235\n",
            "Q 73+374  T 447  ☑ 447 \n",
            "Q 66+166  T 232  ☑ 232 \n",
            "Q 57+90   T 147  ☑ 147 \n",
            "Q 294+531 T 825  ☑ 825 \n",
            "Q 8+981   T 989  ☑ 989 \n",
            "Q 420+302 T 722  ☑ 722 \n",
            "\n",
            "Iteration 29\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0052 - val_accuracy: 0.9991\n",
            "Q 91+758  T 849  ☑ 849 \n",
            "Q 565+10  T 575  ☑ 575 \n",
            "Q 417+187 T 604  ☑ 604 \n",
            "Q 32+848  T 880  ☑ 880 \n",
            "Q 456+16  T 472  ☑ 472 \n",
            "Q 471+111 T 582  ☑ 582 \n",
            "Q 439+37  T 476  ☑ 476 \n",
            "Q 227+50  T 277  ☑ 277 \n",
            "Q 58+47   T 105  ☑ 105 \n",
            "Q 72+370  T 442  ☑ 442 \n"
          ]
        }
      ],
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        if correct == guess:\n",
        "            print(\"☑ \" + guess)\n",
        "        else:\n",
        "            print(\"☒ \" + guess)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vRlH_Rb3uU5Q"
      },
      "source": [
        "You'll get to 99+% validation accuracy after ~30 epochs.\n",
        "\n",
        "Example available on HuggingFace.\n",
        "\n",
        "| Trained Model | Demo |\n",
        "| :--: | :--: |\n",
        "| [![Generic badge](https://img.shields.io/badge/🤗%20Model-Addition%20LSTM-black.svg)](https://huggingface.co/keras-io/addition-lstm) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-Addition%20LSTM-black.svg)](https://huggingface.co/spaces/keras-io/addition-lstm) |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "addition_rnn",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
