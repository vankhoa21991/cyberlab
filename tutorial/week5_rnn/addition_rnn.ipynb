{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sn6bc1lLuU5I"
      },
      "source": [
        "# Sequence to sequence learning for performing number addition\n",
        "\n",
        "**Author:** [Smerity](https://twitter.com/Smerity) and others<br>\n",
        "**Date created:** 2015/08/17<br>\n",
        "**Last modified:** 2020/04/17<br>\n",
        "**Description:** A model that learns to add strings of numbers, e.g. \"535+61\" -> \"596\"."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WYEI30Q4uU5L"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we train a model to learn to add two numbers, provided as strings.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- Input: \"535+61\"\n",
        "- Output: \"596\"\n",
        "\n",
        "Input may optionally be reversed, which was shown to increase performance in many tasks\n",
        " in: [Learning to Execute](http://arxiv.org/abs/1410.4615) and\n",
        "[Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).\n",
        "\n",
        "Theoretically, sequence order inversion introduces shorter term dependencies between\n",
        " source and target for this problem.\n",
        "\n",
        "**Results:**\n",
        "\n",
        "For two digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
        "\n",
        "Three digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
        "\n",
        "Four digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
        "\n",
        "Five digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zgMBB3KhuU5M"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8owtqDWXuU5M"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_xDAsRytuU5N"
      },
      "source": [
        "## Generate the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X2xvbLwuuU5N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total questions: 50000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class CharacterTable:\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = \"0123456789+ \"\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(\n",
        "        \"\".join(\n",
        "            np.random.choice(list(\"0123456789\"))\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))\n",
        "        )\n",
        "    )\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print(\"Total questions:\", len(questions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F5vf35k7uU5O"
      },
      "source": [
        "## Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GUmTANdQuU5O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorization...\n",
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ]
        }
      ],
      "source": [
        "print(\"Vectorization...\")\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5eQ1mVZuU5P"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h87cI6xauU5P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4, 12)             1548      \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print(\"Build model...\")\n",
        "num_layers = 1  # Try to add more LSTM layers!\n",
        "\n",
        "model = keras.Sequential()\n",
        "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
        "# As the decoder RNN's input, repeatedly provide with the last output of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(num_layers):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(layers.LSTM(128, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yzLdCBVOuU5P"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nygLct7SuU5P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.7397 - accuracy: 0.3625 - val_loss: 1.5305 - val_accuracy: 0.4261\n",
            "Q 835+646 T 1481 â˜’ 1414\n",
            "Q 254+95  T 349  â˜’ 559 \n",
            "Q 5+983   T 988  â˜’ 990 \n",
            "Q 744+431 T 1175 â˜’ 1214\n",
            "Q 538+41  T 579  â˜’ 577 \n",
            "Q 329+76  T 405  â˜’ 399 \n",
            "Q 813+256 T 1069 â˜’ 1010\n",
            "Q 64+282  T 346  â˜’ 377 \n",
            "Q 26+940  T 966  â˜’ 904 \n",
            "Q 74+173  T 247  â˜’ 277 \n",
            "\n",
            "Iteration 2\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3248 - accuracy: 0.5034 - val_loss: 1.1778 - val_accuracy: 0.5602\n",
            "Q 42+988  T 1030 â˜’ 1037\n",
            "Q 491+590 T 1081 â˜’ 100 \n",
            "Q 706+23  T 729  â˜’ 737 \n",
            "Q 82+36   T 118  â˜’ 110 \n",
            "Q 604+12  T 616  â˜’ 620 \n",
            "Q 264+2   T 266  â˜’ 268 \n",
            "Q 273+485 T 758  â˜’ 867 \n",
            "Q 144+53  T 197  â˜’ 183 \n",
            "Q 139+0   T 139  â˜’ 137 \n",
            "Q 16+2    T 18   â˜’ 35  \n",
            "\n",
            "Iteration 3\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 1.0532 - accuracy: 0.6088 - val_loss: 0.9573 - val_accuracy: 0.6475\n",
            "Q 1+387   T 388  â˜’ 395 \n",
            "Q 491+556 T 1047 â˜’ 1089\n",
            "Q 50+552  T 602  â˜’ 698 \n",
            "Q 268+84  T 352  â˜’ 355 \n",
            "Q 72+940  T 1012 â˜’ 1004\n",
            "Q 990+8   T 998  â˜’ 990 \n",
            "Q 549+5   T 554  â˜’ 552 \n",
            "Q 532+970 T 1502 â˜’ 1519\n",
            "Q 32+19   T 51   â˜’ 40  \n",
            "Q 99+399  T 498  â˜’ 468 \n",
            "\n",
            "Iteration 4\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.8952 - accuracy: 0.6700 - val_loss: 0.8516 - val_accuracy: 0.6762\n",
            "Q 426+3   T 429  â˜’ 427 \n",
            "Q 418+28  T 446  â˜’ 440 \n",
            "Q 191+30  T 221  â˜’ 223 \n",
            "Q 70+814  T 884  â˜’ 871 \n",
            "Q 108+587 T 695  â˜’ 690 \n",
            "Q 978+64  T 1042 â˜’ 1030\n",
            "Q 442+11  T 453  â˜’ 457 \n",
            "Q 740+266 T 1006 â˜’ 1000\n",
            "Q 65+262  T 327  â˜’ 329 \n",
            "Q 74+560  T 634  â˜’ 627 \n",
            "\n",
            "Iteration 5\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.7858 - accuracy: 0.7128 - val_loss: 0.7417 - val_accuracy: 0.7278\n",
            "Q 333+0   T 333  â˜‘ 333 \n",
            "Q 235+1   T 236  â˜‘ 236 \n",
            "Q 478+61  T 539  â˜’ 531 \n",
            "Q 456+211 T 667  â˜’ 677 \n",
            "Q 81+866  T 947  â˜’ 945 \n",
            "Q 304+278 T 582  â˜’ 588 \n",
            "Q 893+80  T 973  â˜’ 972 \n",
            "Q 581+178 T 759  â˜’ 757 \n",
            "Q 23+989  T 1012 â˜’ 1018\n",
            "Q 81+951  T 1032 â˜’ 1021\n",
            "\n",
            "Iteration 6\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.6966 - accuracy: 0.7465 - val_loss: 0.6514 - val_accuracy: 0.7575\n",
            "Q 19+114  T 133  â˜’ 139 \n",
            "Q 948+846 T 1794 â˜’ 1797\n",
            "Q 784+653 T 1437 â˜’ 1435\n",
            "Q 33+571  T 604  â˜‘ 604 \n",
            "Q 797+84  T 881  â˜’ 872 \n",
            "Q 33+804  T 837  â˜’ 846 \n",
            "Q 417+754 T 1171 â˜’ 1177\n",
            "Q 521+20  T 541  â˜’ 543 \n",
            "Q 30+924  T 954  â˜’ 952 \n",
            "Q 441+76  T 517  â˜’ 519 \n",
            "\n",
            "Iteration 7\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.5096 - accuracy: 0.8128 - val_loss: 0.3941 - val_accuracy: 0.8609\n",
            "Q 703+297 T 1000 â˜’ 900 \n",
            "Q 69+329  T 398  â˜’ 397 \n",
            "Q 96+615  T 711  â˜‘ 711 \n",
            "Q 3+258   T 261  â˜‘ 261 \n",
            "Q 577+240 T 817  â˜’ 818 \n",
            "Q 947+996 T 1943 â˜’ 1944\n",
            "Q 314+721 T 1035 â˜‘ 1035\n",
            "Q 30+69   T 99   â˜’ 90  \n",
            "Q 466+727 T 1193 â˜’ 1192\n",
            "Q 4+181   T 185  â˜‘ 185 \n",
            "\n",
            "Iteration 8\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.2670 - accuracy: 0.9192 - val_loss: 0.2521 - val_accuracy: 0.9150\n",
            "Q 628+2   T 630  â˜‘ 630 \n",
            "Q 92+333  T 425  â˜‘ 425 \n",
            "Q 45+668  T 713  â˜‘ 713 \n",
            "Q 61+71   T 132  â˜‘ 132 \n",
            "Q 264+16  T 280  â˜‘ 280 \n",
            "Q 143+466 T 609  â˜‘ 609 \n",
            "Q 97+56   T 153  â˜‘ 153 \n",
            "Q 944+221 T 1165 â˜‘ 1165\n",
            "Q 840+830 T 1670 â˜’ 1660\n",
            "Q 943+614 T 1557 â˜‘ 1557\n",
            "\n",
            "Iteration 9\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.1398 - accuracy: 0.9659 - val_loss: 0.1099 - val_accuracy: 0.9717\n",
            "Q 2+960   T 962  â˜’ 963 \n",
            "Q 4+115   T 119  â˜‘ 119 \n",
            "Q 655+960 T 1615 â˜‘ 1615\n",
            "Q 88+403  T 491  â˜‘ 491 \n",
            "Q 94+22   T 116  â˜‘ 116 \n",
            "Q 4+323   T 327  â˜‘ 327 \n",
            "Q 790+134 T 924  â˜‘ 924 \n",
            "Q 30+705  T 735  â˜‘ 735 \n",
            "Q 532+970 T 1502 â˜‘ 1502\n",
            "Q 858+8   T 866  â˜‘ 866 \n",
            "\n",
            "Iteration 10\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0897 - accuracy: 0.9779 - val_loss: 0.0762 - val_accuracy: 0.9796\n",
            "Q 845+5   T 850  â˜‘ 850 \n",
            "Q 624+427 T 1051 â˜‘ 1051\n",
            "Q 410+18  T 428  â˜‘ 428 \n",
            "Q 246+352 T 598  â˜‘ 598 \n",
            "Q 92+850  T 942  â˜‘ 942 \n",
            "Q 23+250  T 273  â˜‘ 273 \n",
            "Q 240+34  T 274  â˜‘ 274 \n",
            "Q 449+351 T 800  â˜‘ 800 \n",
            "Q 31+526  T 557  â˜‘ 557 \n",
            "Q 22+375  T 397  â˜‘ 397 \n",
            "\n",
            "Iteration 11\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0622 - accuracy: 0.9849 - val_loss: 0.0427 - val_accuracy: 0.9905\n",
            "Q 226+3   T 229  â˜‘ 229 \n",
            "Q 875+513 T 1388 â˜’ 1288\n",
            "Q 884+5   T 889  â˜‘ 889 \n",
            "Q 484+803 T 1287 â˜‘ 1287\n",
            "Q 26+104  T 130  â˜‘ 130 \n",
            "Q 472+21  T 493  â˜‘ 493 \n",
            "Q 93+419  T 512  â˜‘ 512 \n",
            "Q 26+619  T 645  â˜‘ 645 \n",
            "Q 744+498 T 1242 â˜‘ 1242\n",
            "Q 374+278 T 652  â˜‘ 652 \n",
            "\n",
            "Iteration 12\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0477 - accuracy: 0.9879 - val_loss: 0.0560 - val_accuracy: 0.9825\n",
            "Q 537+584 T 1121 â˜‘ 1121\n",
            "Q 436+669 T 1105 â˜‘ 1105\n",
            "Q 73+478  T 551  â˜‘ 551 \n",
            "Q 336+244 T 580  â˜‘ 580 \n",
            "Q 112+44  T 156  â˜‘ 156 \n",
            "Q 833+10  T 843  â˜‘ 843 \n",
            "Q 215+60  T 275  â˜‘ 275 \n",
            "Q 885+18  T 903  â˜‘ 903 \n",
            "Q 947+3   T 950  â˜‘ 950 \n",
            "Q 756+403 T 1159 â˜‘ 1159\n",
            "\n",
            "Iteration 13\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0369 - accuracy: 0.9904 - val_loss: 0.0305 - val_accuracy: 0.9923\n",
            "Q 97+34   T 131  â˜‘ 131 \n",
            "Q 901+487 T 1388 â˜‘ 1388\n",
            "Q 232+27  T 259  â˜‘ 259 \n",
            "Q 255+474 T 729  â˜‘ 729 \n",
            "Q 24+63   T 87   â˜‘ 87  \n",
            "Q 358+537 T 895  â˜‘ 895 \n",
            "Q 93+494  T 587  â˜‘ 587 \n",
            "Q 391+53  T 444  â˜‘ 444 \n",
            "Q 199+62  T 261  â˜‘ 261 \n",
            "Q 87+657  T 744  â˜‘ 744 \n",
            "\n",
            "Iteration 14\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0434 - accuracy: 0.9881 - val_loss: 0.0264 - val_accuracy: 0.9931\n",
            "Q 72+299  T 371  â˜’ 381 \n",
            "Q 489+489 T 978  â˜‘ 978 \n",
            "Q 46+891  T 937  â˜‘ 937 \n",
            "Q 461+7   T 468  â˜‘ 468 \n",
            "Q 65+34   T 99   â˜‘ 99  \n",
            "Q 60+34   T 94   â˜‘ 94  \n",
            "Q 718+18  T 736  â˜‘ 736 \n",
            "Q 104+43  T 147  â˜‘ 147 \n",
            "Q 38+579  T 617  â˜‘ 617 \n",
            "Q 819+29  T 848  â˜‘ 848 \n",
            "\n",
            "Iteration 15\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0214 - accuracy: 0.9950 - val_loss: 0.0722 - val_accuracy: 0.9765\n",
            "Q 804+50  T 854  â˜‘ 854 \n",
            "Q 80+633  T 713  â˜‘ 713 \n",
            "Q 22+50   T 72   â˜‘ 72  \n",
            "Q 521+739 T 1260 â˜‘ 1260\n",
            "Q 1+297   T 298  â˜’ 398 \n",
            "Q 90+19   T 109  â˜’ 119 \n",
            "Q 22+375  T 397  â˜‘ 397 \n",
            "Q 93+201  T 294  â˜‘ 294 \n",
            "Q 88+453  T 541  â˜‘ 541 \n",
            "Q 557+281 T 838  â˜‘ 838 \n",
            "\n",
            "Iteration 16\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0365 - accuracy: 0.9898 - val_loss: 0.0203 - val_accuracy: 0.9948\n",
            "Q 91+220  T 311  â˜‘ 311 \n",
            "Q 56+604  T 660  â˜‘ 660 \n",
            "Q 267+69  T 336  â˜‘ 336 \n",
            "Q 569+49  T 618  â˜‘ 618 \n",
            "Q 94+964  T 1058 â˜‘ 1058\n",
            "Q 806+961 T 1767 â˜‘ 1767\n",
            "Q 9+704   T 713  â˜‘ 713 \n",
            "Q 5+519   T 524  â˜‘ 524 \n",
            "Q 170+432 T 602  â˜‘ 602 \n",
            "Q 83+174  T 257  â˜‘ 257 \n",
            "\n",
            "Iteration 17\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0306 - accuracy: 0.9919 - val_loss: 0.0602 - val_accuracy: 0.9812\n",
            "Q 502+27  T 529  â˜‘ 529 \n",
            "Q 92+971  T 1063 â˜‘ 1063\n",
            "Q 274+163 T 437  â˜‘ 437 \n",
            "Q 3+260   T 263  â˜‘ 263 \n",
            "Q 292+795 T 1087 â˜‘ 1087\n",
            "Q 2+598   T 600  â˜‘ 600 \n",
            "Q 521+337 T 858  â˜‘ 858 \n",
            "Q 353+533 T 886  â˜‘ 886 \n",
            "Q 88+665  T 753  â˜‘ 753 \n",
            "Q 1+156   T 157  â˜‘ 157 \n",
            "\n",
            "Iteration 18\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0231 - accuracy: 0.9937 - val_loss: 0.0156 - val_accuracy: 0.9966\n",
            "Q 456+71  T 527  â˜‘ 527 \n",
            "Q 713+357 T 1070 â˜‘ 1070\n",
            "Q 9+704   T 713  â˜‘ 713 \n",
            "Q 86+745  T 831  â˜‘ 831 \n",
            "Q 5+18    T 23   â˜‘ 23  \n",
            "Q 28+267  T 295  â˜‘ 295 \n",
            "Q 319+8   T 327  â˜‘ 327 \n",
            "Q 60+863  T 923  â˜‘ 923 \n",
            "Q 277+361 T 638  â˜‘ 638 \n",
            "Q 24+69   T 93   â˜‘ 93  \n",
            "\n",
            "Iteration 19\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0241 - accuracy: 0.9935 - val_loss: 0.0297 - val_accuracy: 0.9912\n",
            "Q 851+1   T 852  â˜‘ 852 \n",
            "Q 411+957 T 1368 â˜‘ 1368\n",
            "Q 666+82  T 748  â˜‘ 748 \n",
            "Q 985+0   T 985  â˜‘ 985 \n",
            "Q 969+9   T 978  â˜‘ 978 \n",
            "Q 686+928 T 1614 â˜‘ 1614\n",
            "Q 415+91  T 506  â˜‘ 506 \n",
            "Q 9+412   T 421  â˜‘ 421 \n",
            "Q 418+28  T 446  â˜‘ 446 \n",
            "Q 829+38  T 867  â˜‘ 867 \n",
            "\n",
            "Iteration 20\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0137 - accuracy: 0.9967 - val_loss: 0.0702 - val_accuracy: 0.9790\n",
            "Q 17+955  T 972  â˜‘ 972 \n",
            "Q 428+570 T 998  â˜‘ 998 \n",
            "Q 453+370 T 823  â˜‘ 823 \n",
            "Q 545+17  T 562  â˜‘ 562 \n",
            "Q 80+780  T 860  â˜‘ 860 \n",
            "Q 93+725  T 818  â˜‘ 818 \n",
            "Q 581+55  T 636  â˜‘ 636 \n",
            "Q 216+60  T 276  â˜‘ 276 \n",
            "Q 96+960  T 1056 â˜‘ 1056\n",
            "Q 1+642   T 643  â˜‘ 643 \n",
            "\n",
            "Iteration 21\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0202 - accuracy: 0.9945 - val_loss: 0.0101 - val_accuracy: 0.9973\n",
            "Q 521+20  T 541  â˜‘ 541 \n",
            "Q 2+42    T 44   â˜‘ 44  \n",
            "Q 868+934 T 1802 â˜‘ 1802\n",
            "Q 9+129   T 138  â˜‘ 138 \n",
            "Q 178+655 T 833  â˜‘ 833 \n",
            "Q 51+97   T 148  â˜‘ 148 \n",
            "Q 492+80  T 572  â˜‘ 572 \n",
            "Q 835+646 T 1481 â˜‘ 1481\n",
            "Q 838+841 T 1679 â˜‘ 1679\n",
            "Q 45+547  T 592  â˜‘ 592 \n",
            "\n",
            "Iteration 22\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0157 - accuracy: 0.9957 - val_loss: 0.0332 - val_accuracy: 0.9899\n",
            "Q 73+65   T 138  â˜‘ 138 \n",
            "Q 289+92  T 381  â˜‘ 381 \n",
            "Q 148+72  T 220  â˜‘ 220 \n",
            "Q 887+401 T 1288 â˜‘ 1288\n",
            "Q 806+675 T 1481 â˜‘ 1481\n",
            "Q 170+6   T 176  â˜‘ 176 \n",
            "Q 2+598   T 600  â˜‘ 600 \n",
            "Q 490+68  T 558  â˜‘ 558 \n",
            "Q 587+860 T 1447 â˜‘ 1447\n",
            "Q 208+261 T 469  â˜‘ 469 \n",
            "\n",
            "Iteration 23\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0149 - accuracy: 0.9962 - val_loss: 0.0198 - val_accuracy: 0.9934\n",
            "Q 704+529 T 1233 â˜’ 1333\n",
            "Q 65+34   T 99   â˜‘ 99  \n",
            "Q 80+99   T 179  â˜‘ 179 \n",
            "Q 2+255   T 257  â˜‘ 257 \n",
            "Q 31+527  T 558  â˜‘ 558 \n",
            "Q 771+87  T 858  â˜‘ 858 \n",
            "Q 825+44  T 869  â˜‘ 869 \n",
            "Q 903+893 T 1796 â˜‘ 1796\n",
            "Q 357+20  T 377  â˜‘ 377 \n",
            "Q 792+568 T 1360 â˜‘ 1360\n",
            "\n",
            "Iteration 24\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0235 - accuracy: 0.9931 - val_loss: 0.0370 - val_accuracy: 0.9872\n",
            "Q 95+481  T 576  â˜‘ 576 \n",
            "Q 128+10  T 138  â˜‘ 138 \n",
            "Q 463+85  T 548  â˜‘ 548 \n",
            "Q 289+375 T 664  â˜‘ 664 \n",
            "Q 61+71   T 132  â˜‘ 132 \n",
            "Q 221+1   T 222  â˜‘ 222 \n",
            "Q 7+55    T 62   â˜‘ 62  \n",
            "Q 522+43  T 565  â˜‘ 565 \n",
            "Q 583+554 T 1137 â˜‘ 1137\n",
            "Q 70+423  T 493  â˜‘ 493 \n",
            "\n",
            "Iteration 25\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0081 - accuracy: 0.9982 - val_loss: 0.0037 - val_accuracy: 0.9992\n",
            "Q 148+257 T 405  â˜‘ 405 \n",
            "Q 302+6   T 308  â˜‘ 308 \n",
            "Q 456+559 T 1015 â˜‘ 1015\n",
            "Q 77+221  T 298  â˜‘ 298 \n",
            "Q 92+850  T 942  â˜‘ 942 \n",
            "Q 82+222  T 304  â˜‘ 304 \n",
            "Q 593+2   T 595  â˜‘ 595 \n",
            "Q 319+537 T 856  â˜‘ 856 \n",
            "Q 97+768  T 865  â˜‘ 865 \n",
            "Q 328+0   T 328  â˜‘ 328 \n",
            "\n",
            "Iteration 26\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0197 - accuracy: 0.9945 - val_loss: 0.0057 - val_accuracy: 0.9987\n",
            "Q 438+13  T 451  â˜‘ 451 \n",
            "Q 938+14  T 952  â˜‘ 952 \n",
            "Q 20+778  T 798  â˜‘ 798 \n",
            "Q 835+53  T 888  â˜‘ 888 \n",
            "Q 68+245  T 313  â˜‘ 313 \n",
            "Q 39+687  T 726  â˜‘ 726 \n",
            "Q 690+75  T 765  â˜‘ 765 \n",
            "Q 78+974  T 1052 â˜‘ 1052\n",
            "Q 93+433  T 526  â˜‘ 526 \n",
            "Q 554+51  T 605  â˜‘ 605 \n",
            "\n",
            "Iteration 27\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0595 - val_accuracy: 0.9791\n",
            "Q 930+89  T 1019 â˜‘ 1019\n",
            "Q 5+460   T 465  â˜‘ 465 \n",
            "Q 3+994   T 997  â˜’ 996 \n",
            "Q 694+760 T 1454 â˜’ 1464\n",
            "Q 629+7   T 636  â˜‘ 636 \n",
            "Q 65+879  T 944  â˜‘ 944 \n",
            "Q 67+796  T 863  â˜‘ 863 \n",
            "Q 70+420  T 490  â˜’ 400 \n",
            "Q 338+361 T 699  â˜‘ 699 \n",
            "Q 674+14  T 688  â˜‘ 688 \n",
            "\n",
            "Iteration 28\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.0178 - val_accuracy: 0.9945\n",
            "Q 971+223 T 1194 â˜‘ 1194\n",
            "Q 92+434  T 526  â˜‘ 526 \n",
            "Q 77+913  T 990  â˜‘ 990 \n",
            "Q 959+276 T 1235 â˜‘ 1235\n",
            "Q 73+374  T 447  â˜‘ 447 \n",
            "Q 66+166  T 232  â˜‘ 232 \n",
            "Q 57+90   T 147  â˜‘ 147 \n",
            "Q 294+531 T 825  â˜‘ 825 \n",
            "Q 8+981   T 989  â˜‘ 989 \n",
            "Q 420+302 T 722  â˜‘ 722 \n",
            "\n",
            "Iteration 29\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0052 - val_accuracy: 0.9991\n",
            "Q 91+758  T 849  â˜‘ 849 \n",
            "Q 565+10  T 575  â˜‘ 575 \n",
            "Q 417+187 T 604  â˜‘ 604 \n",
            "Q 32+848  T 880  â˜‘ 880 \n",
            "Q 456+16  T 472  â˜‘ 472 \n",
            "Q 471+111 T 582  â˜‘ 582 \n",
            "Q 439+37  T 476  â˜‘ 476 \n",
            "Q 227+50  T 277  â˜‘ 277 \n",
            "Q 58+47   T 105  â˜‘ 105 \n",
            "Q 72+370  T 442  â˜‘ 442 \n"
          ]
        }
      ],
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        if correct == guess:\n",
        "            print(\"â˜‘ \" + guess)\n",
        "        else:\n",
        "            print(\"â˜’ \" + guess)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vRlH_Rb3uU5Q"
      },
      "source": [
        "You'll get to 99+% validation accuracy after ~30 epochs.\n",
        "\n",
        "Example available on HuggingFace.\n",
        "\n",
        "| Trained Model | Demo |\n",
        "| :--: | :--: |\n",
        "| [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Model-Addition%20LSTM-black.svg)](https://huggingface.co/keras-io/addition-lstm) | [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Spaces-Addition%20LSTM-black.svg)](https://huggingface.co/spaces/keras-io/addition-lstm) |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "addition_rnn",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
