{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Các cấu trúc RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                torch.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # Update gate parameters\n",
    "    W_xr, W_hr, b_r = three()  # Reset gate parameters\n",
    "    W_xh, W_hh, b_h = three()  # Candidate hidden state parameters\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # Attach gradients\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "def init_gru_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n",
    "        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n",
    "        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = H @ W_hq + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                torch.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # Input gate parameters\n",
    "    W_xf, W_hf, b_f = three()  # Forget gate parameters\n",
    "    W_xo, W_ho, b_o = three()  # Output gate parameters\n",
    "    W_xc, W_hc, b_c = three()  # Candidate memory cell parameters\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # Attach gradients\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),\n",
    "            torch.zeros((batch_size, num_hiddens), device=device))\n",
    "    \n",
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)\n",
    "        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)\n",
    "        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)\n",
    "        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * torch.tanh(C)\n",
    "        Y = (H @ W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: dùng GRU hoặc LSTM để huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seq2seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"The base decoder interface for the encoder-decoder architecture.\n",
    "\n",
    "    Defined in :numref:`sec_encoder-decoder`\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
    "        X = self.embedding(X)\n",
    "        # In RNN models, the first axis corresponds to time steps\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # When state is not mentioned, it defaults to zeros\n",
    "        output, state = self.rnn(X)\n",
    "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
    "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "encoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        # Broadcast `context` so it has the same `num_steps` as `X`\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Hàm loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, 4)\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Huấn luyện mô hình seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    #animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "    #                        xlim=[10, num_epochs])\n",
    "    epochs = []\n",
    "    metrics = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        #timer = d2l.Timer()\n",
    "        tik = time.time()\n",
    "        #metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "        metric = [0.0] * 2  # Sum of training loss, no. of tokens\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                               device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()  # Make the loss scalar for `backward`\n",
    "            grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                #metric.add(l.sum(), num_tokens)\n",
    "                metric = [a + float(b) for a, b in zip(metric, (l.sum(), num_tokens))]\n",
    "                \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            epochs.append(epoch+1)\n",
    "            metrics.append(metric[0] / metric[1])\n",
    "            #animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "            \n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / (time.time()-tik):.1f} '\n",
    "          f'tokens/sec on {str(device)}')\n",
    "    \n",
    "    fig, axes = plt.subplots()\n",
    "    axes.plot(epochs, metrics)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mo hinh EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"The base class for the encoder-decoder architecture.\n",
    "\n",
    "    Defined in :numref:`sec_encoder-decoder`\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:23<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.019, 32792.3 tokens/sec on cuda\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import load_data_nmt\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr, num_epochs = 0.005, 300\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(\n",
    "    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(\n",
    "    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import truncate_pad\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
    "    # Set `net` to eval mode for inference\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # Add the batch axis\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # Add the batch axis\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Save attention weights (to be covered later)\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # Once the end-of-sequence token is predicted, the generation of the\n",
    "        # output sequence is complete\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('je suis bien .', [])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_steps = 100\n",
    "src_sentence = 'I am here' \n",
    "predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5980b3a08a94ca7d4924634a4e2ea7a5238b35723e0e305f23d28d43ee7ce05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
