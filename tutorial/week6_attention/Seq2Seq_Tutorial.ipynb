{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh9tOmc3hCjO"
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMy-3z6tu9VW"
      },
      "source": [
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "\n",
        "def make_batch():\n",
        "  input_batch, output_batch, target_batch = [], [], []\n",
        "\n",
        "  for seq in seq_data:\n",
        "    for i in range(2):\n",
        "        seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
        "\n",
        "    input = [num_dic[n] for n in seq[0]]\n",
        "    output = [num_dic[n] for n in ('S' + seq[1])]\n",
        "    target = [num_dic[n] for n in (seq[1] + 'E')]\n",
        "\n",
        "    input_batch.append(np.eye(n_class)[input])\n",
        "    output_batch.append(np.eye(n_class)[output])\n",
        "    target_batch.append(target) # not one-hot\n",
        "\n",
        "  # make tensor\n",
        "  return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWXP30BcuyUp"
      },
      "source": [
        "# Model\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "\n",
        "    self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "    self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "    self.fc = nn.Linear(n_hidden, n_class)\n",
        "\n",
        "  def forward(self, enc_input, enc_hidden, dec_input):\n",
        "    enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, time step), batch_size, n_class]\n",
        "    dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "    # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
        "    # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]\n",
        "    outputs, _ = self.dec_cell(dec_input, enc_states)\n",
        "\n",
        "    model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class]\n",
        "    return model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H3tLmOWutZh",
        "outputId": "f9c602d0-989d-4666-c97a-9b314ed1f2c6"
      },
      "source": [
        "n_step = 5\n",
        "n_hidden = 128\n",
        "\n",
        "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
        "\n",
        "n_class = len(num_dic)\n",
        "batch_size = len(seq_data)\n",
        "\n",
        "model = Seq2Seq()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "for epoch in range(5000):\n",
        "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, batch_size, n_hidden)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # input_batch : [batch_size, max_len(=n_step, time step), n_class]\n",
        "    # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\n",
        "    # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\n",
        "    output = model(input_batch, hidden, output_batch)\n",
        "    # output : [max_len+1, batch_size, n_class]\n",
        "    output = output.transpose(0, 1) # [batch_size, max_len+1(=6), n_class]\n",
        "    loss = 0\n",
        "    for i in range(0, len(target_batch)):\n",
        "        # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n",
        "        loss += criterion(output[i], target_batch[i])\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "<ipython-input-26-8a437a402612>:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1000 cost = 0.003759\n",
            "Epoch: 2000 cost = 0.001021\n",
            "Epoch: 3000 cost = 0.000434\n",
            "Epoch: 4000 cost = 0.000217\n",
            "Epoch: 5000 cost = 0.000117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmuMw0yEoJ43",
        "outputId": "886bde19-a8ea-4503-b0ad-4b485f5faf2a"
      },
      "source": [
        "input_batch, output_batch, _ = make_batch()\n",
        "\n",
        "# make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "hidden = torch.zeros(1, 6, 128)\n",
        "for i in range(0, len(input_batch)):\n",
        "  output = model(input_batch, hidden, output_batch)\n",
        "  # output : [max_len+1(=6), batch_size(=1), n_class]\n",
        "\n",
        "  output = output.transpose(0, 1)\n",
        "  output = torch.argmax(output.data, -1)[i].numpy()\n",
        "\n",
        "  decode = [char_arr[c] for c in output]\n",
        "\n",
        "  end = decode.index('E')\n",
        "  translated = ''.join(decode[:end])\n",
        "\n",
        "  input_word = [char_arr[w] for w in input_batch[i].max(-1)[1]]\n",
        "  input_word = ''.join(input_word[:end])\n",
        "  print(f\"{input_word.replace('P', '')} -> {translated.replace('P', '')}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man -> women\n",
            "black -> white\n",
            "king -> queen\n",
            "girl -> boy\n",
            "up -> down\n",
            "high -> low\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyuOu6FC5gKv"
      },
      "source": [
        "# Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0jeMGxxvz9F"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "\n",
        "%matplotlib inline\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDMv2gVyYhwa",
        "outputId": "75be300a-b6a8-4c95-c524-d829e3ebd792"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/devm2024/nmt_keras/master/fra.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-22 20:39:04--  https://raw.githubusercontent.com/devm2024/nmt_keras/master/fra.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10481325 (10.0M) [text/plain]\n",
            "Saving to: ‘fra.txt’\n",
            "\n",
            "fra.txt             100%[===================>]  10.00M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-06-22 20:39:05 (184 MB/s) - ‘fra.txt’ saved [10481325/10481325]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRRnMSrHYvBR"
      },
      "source": [
        "lines= pd.read_table('fra.txt', names=['eng', 'fr'])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Divdhh6ZLb6"
      },
      "source": [
        "lines = lines[:400]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hBVCPZnDZOfL",
        "outputId": "386dbc3d-1b89-49b1-d704-6d8305fb9c4f"
      },
      "source": [
        "def preprocess():\n",
        "  lines.eng=lines.eng.apply(lambda x: x.lower())\n",
        "  lines.fr=lines.fr.apply(lambda x: x.lower())\n",
        "  lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
        "  lines.fr=lines.fr.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
        "  exclude = set(string.punctuation)\n",
        "  lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "  lines.fr=lines.fr.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "  remove_digits = str.maketrans('', '', digits)\n",
        "  lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
        "  lines.fr=lines.fr.apply(lambda x: x.translate(remove_digits))\n",
        "  lines.fr = lines.fr.apply(lambda x : 'START_ '+ x + ' _END')\n",
        "\n",
        "preprocess()\n",
        "lines.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    eng                     fr\n",
              "0    go        START_ va  _END\n",
              "1   run     START_ cours  _END\n",
              "2   run    START_ courez  _END\n",
              "3   wow  START_ ça alors  _END\n",
              "4  fire    START_ au feu  _END"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ec67bda-713f-4a28-a2d1-ed054fe98adc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>go</td>\n",
              "      <td>START_ va  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>run</td>\n",
              "      <td>START_ cours  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>run</td>\n",
              "      <td>START_ courez  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wow</td>\n",
              "      <td>START_ ça alors  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fire</td>\n",
              "      <td>START_ au feu  _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ec67bda-713f-4a28-a2d1-ed054fe98adc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ec67bda-713f-4a28-a2d1-ed054fe98adc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ec67bda-713f-4a28-a2d1-ed054fe98adc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "729cFdyZZwTp"
      },
      "source": [
        "all_eng_words=set()\n",
        "for eng in lines.eng:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "\n",
        "all_french_words=set()\n",
        "for fr in lines.fr:\n",
        "    for word in fr.split():\n",
        "        if word not in all_french_words:\n",
        "            all_french_words.add(word)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMWOffciZzrV",
        "outputId": "45d816cb-81b2-4ef2-a81f-f6451636e40d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_words = sorted(list(all_eng_words))\n",
        "target_words = sorted(list(all_french_words))\n",
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_french_words)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "print(num_encoder_tokens)\n",
        "print(num_decoder_tokens)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "178\n",
            "395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Bzm852aFpC"
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_token_index)"
      ],
      "metadata": {
        "id": "GKOGh342cyaU",
        "outputId": "45715879-6538-493c-bd9d-c8c302fc9f6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'COMMA': 0, 'a': 1, 'agree': 2, 'ahead': 3, 'am': 4, 'ask': 5, 'attack': 6, 'away': 7, 'awesome': 8, 'back': 9, 'bald': 10, 'be': 11, 'beat': 12, 'beats': 13, 'busy': 14, 'call': 15, 'calm': 16, 'came': 17, 'cheer': 18, 'cheers': 19, 'cold': 20, 'come': 21, 'cool': 22, 'cuff': 23, 'cute': 24, 'deaf': 25, 'deep': 26, 'did': 27, 'died': 28, 'done': 29, 'down': 30, 'drive': 31, 'drop': 32, 'failed': 33, 'fair': 34, 'fast': 35, 'fat': 36, 'fell': 37, 'find': 38, 'fine': 39, 'fire': 40, 'fit': 41, 'forgot': 42, 'free': 43, 'full': 44, 'fun': 45, 'get': 46, 'glad': 47, 'go': 48, 'good': 49, 'goodbye': 50, 'got': 51, 'grab': 52, 'guys': 53, 'hang': 54, 'hard': 55, 'have': 56, 'he': 57, 'help': 58, 'helps': 59, 'here': 60, 'hes': 61, 'hi': 62, 'him': 63, 'his': 64, 'hit': 65, 'hold': 66, 'home': 67, 'hop': 68, 'how': 69, 'hug': 70, 'humor': 71, 'hurry': 72, 'hurts': 73, 'i': 74, 'ill': 75, 'im': 76, 'in': 77, 'it': 78, 'its': 79, 'ive': 80, 'job': 81, 'join': 82, 'jump': 83, 'keep': 84, 'kind': 85, 'kiss': 86, 'knew': 87, 'know': 88, 'late': 89, 'lazy': 90, 'leave': 91, 'left': 92, 'lets': 93, 'lied': 94, 'lies': 95, 'listen': 96, 'long': 97, 'look': 98, 'lost': 99, 'man': 100, 'marry': 101, 'may': 102, 'me': 103, 'new': 104, 'nice': 105, 'no': 106, 'now': 107, 'odd': 108, 'off': 109, 'oh': 110, 'ok': 111, 'okay': 112, 'on': 113, 'open': 114, 'out': 115, 'paid': 116, 'pay': 117, 'perfect': 118, 'phoned': 119, 'quit': 120, 'real': 121, 'really': 122, 'refuse': 123, 'run': 124, 'runs': 125, 'sad': 126, 'safe': 127, 'save': 128, 'saw': 129, 'see': 130, 'she': 131, 'show': 132, 'shut': 133, 'shy': 134, 'sick': 135, 'sit': 136, 'slow': 137, 'so': 138, 'some': 139, 'speak': 140, 'stayed': 141, 'still': 142, 'stop': 143, 'sure': 144, 'take': 145, 'tall': 146, 'tell': 147, 'terrific': 148, 'thanks': 149, 'them': 150, 'they': 151, 'thin': 152, 'this': 153, 'tidy': 154, 'tom': 155, 'too': 156, 'tried': 157, 'tries': 158, 'trust': 159, 'try': 160, 'ugly': 161, 'up': 162, 'us': 163, 'use': 164, 'wait': 165, 'wake': 166, 'wash': 167, 'way': 168, 'we': 169, 'weak': 170, 'well': 171, 'wet': 172, 'who': 173, 'won': 174, 'works': 175, 'wow': 176, 'you': 177}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJmE4m4saIK1",
        "outputId": "1542b7c2-bee3-4495-fccd-1cf4ef585ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_words)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_words)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-39084f4f41d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m input_token_index = dict(\n\u001b[0;32m----> 2\u001b[0;31m     [(char, i) for i, char in enumerate(input_characters)])\n\u001b[0m\u001b[1;32m      3\u001b[0m target_token_index = dict(\n\u001b[1;32m      4\u001b[0m     [(char, i) for i, char in enumerate(target_characters)])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_characters' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZTyJTLQaws-"
      },
      "source": [],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P784yaiaLlT"
      },
      "source": [
        "embedding_size = 50"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MygkvTYBcWSj"
      },
      "source": [
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "# from keras.utils import plot_model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JypzJp-acayT"
      },
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "en_x=  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n",
        "encoder = LSTM(50, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(en_x)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_H24NDPcdsK"
      },
      "source": [
        "decoder_inputs = Input(shape=(None,))\n",
        "dex=  Embedding(num_decoder_tokens, embedding_size)\n",
        "final_dex= dex(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VWV9drdcqum"
      },
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "-dk2JV4YczbQ",
        "outputId": "64e080c2-88d5-4f70-9211-8c90ae0c59ec"
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=128,\n",
        "          epochs=50,\n",
        "          validation_split=0.05)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1ea34a2606cb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit([encoder_inputs, decoder_inputs], decoder_outputs,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           validation_split=0.05)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1666\u001b[0m     \u001b[0munsplitable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_arrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_can_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munsplitable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1669\u001b[0m             \u001b[0;34m\"`validation_split` is only supported for Tensors or NumPy \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0;34m\"arrays, found following types in the input: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsplitable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'keras.engine.keras_tensor.KerasTensor'>, <class 'keras.engine.keras_tensor.KerasTensor'>, <class 'keras.engine.keras_tensor.KerasTensor'>]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32P9UulaedMV"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f0ekLicc3rp"
      },
      "source": [
        "decoder_state_input_h = Input(shape=(50,))\n",
        "decoder_state_input_c = Input(shape=(50,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "final_dex2= dex(decoder_inputs)\n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HcbL0xydFC2"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 52):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGvawUXqdHur",
        "outputId": "8271dde0-cd20-46be-8dc0-ce05a329c1b9"
      },
      "source": [
        "for seq_index in [99,2,45,40]:\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print(input_seq)\n",
        "    print('Input sentence:', lines.eng[seq_index: seq_index + 1])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "-\n",
            "[[0. 0. 0. 0. 0. 0. 0.]]\n",
            "Input sentence: 99    drop it\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  quelle quelle quelle quelle quelle quelle quelle quelle\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "-\n",
            "[[0. 0. 0. 0. 0. 0. 0.]]\n",
            "Input sentence: 2    run\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  quelle quelle quelle quelle quelle quelle quelle quelle\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "[[0. 0. 0. 0. 0. 0. 0.]]\n",
            "Input sentence: 45    im \n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  quelle quelle quelle quelle quelle quelle quelle quelle\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "-\n",
            "[[0. 0. 0. 0. 0. 0. 0.]]\n",
            "Input sentence: 40    i fell\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  quelle quelle quelle quelle quelle quelle quelle quelle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLq8CFDodNeb"
      },
      "source": [],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m3ved5Af8eN"
      },
      "source": [
        "# Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qVQXeJ1f_BG",
        "outputId": "b96d8acd-f7e5-48ae-920c-2f179db122ec"
      },
      "source": [
        "!pip install tensorflow_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/ed/bbb51e9eccca0c2bfdf9df66e54cdff563b6f32daed9255da9b9a541368f/tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.6,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.34.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.8.0->tensorflow_text) (57.0.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.31.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.5.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.2.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.8)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXqgwa5zgGcb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, GRU, Layer\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEwgF2fzgJ_W",
        "outputId": "3b43c450-d311-4295-8fb6-edaea6fd6a39"
      },
      "source": [
        "# Download the file\n",
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcJ7u575gMJE"
      },
      "source": [
        "def load_data(path):\n",
        "  text = path_to_file.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  inp = [inp for targ, inp in pairs]\n",
        "  targ = [targ for targ, inp in pairs]\n",
        "\n",
        "  return targ, inp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBLzHGf3gOhC",
        "outputId": "fe8c0e92-0bbd-4a3b-e1b2-7433ea8c4fd6"
      },
      "source": [
        "targ, inp = load_data(path_to_file)\n",
        "print(inp[-1])\n",
        "print(targ[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
            "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koxQrXXcgQim"
      },
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKgSPHvrgSh4"
      },
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1aV5JBEgVnb",
        "outputId": "a0e5a5c9-92ac-4aa1-e5fe-94d51963bf29"
      },
      "source": [
        "for spanish, english in dataset.take(1):\n",
        "  pass\n",
        "print(spanish[0].numpy().decode())\n",
        "print(tf_lower_and_split_punct(spanish[0]).numpy().decode())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Es un problema que no podemos resolver.\n",
            "[START] es un problema que no podemos resolver . [END]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgtmpMb9gYOf",
        "outputId": "61221b97-a7d8-4d84-947a-07b259f9fc0d"
      },
      "source": [
        "max_vocab_size = 400\n",
        "\n",
        "input_text_processor = preprocessing.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1gDcDSkgZu5",
        "outputId": "a685e54c-de2a-4ff9-885e-5437620c6d73"
      },
      "source": [
        "output_text_processor = preprocessing.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ZmpCJGgfdW"
      },
      "source": [
        "class Encoder(Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_lang_embedding = Embedding(input_vocab_size, embedding_dim)\n",
        "    self.gru = GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, word_indices, state=None):\n",
        "    word_embeddings =  self.input_lang_embedding(word_indices)\n",
        "    whole_sequence_output, final_state = self.gru(word_embeddings)\n",
        "    return whole_sequence_output, final_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly-gwIhbgnIa"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "      super().__init__()\n",
        "      self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "      self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "  def call(self, query, keys, mask):\n",
        "      query_weights = tf.expand_dims(self.W1(query), 2)\n",
        "      keys_weights = tf.expand_dims(self.W2(keys), 1)\n",
        "      score = tf.reduce_sum(tf.nn.tanh(query_weights + keys_weights), -1)\n",
        "      padding_mask = tf.expand_dims(math_ops.logical_not(mask), 1)\n",
        "      score -= 1e9 * math_ops.cast(padding_mask, dtype=score.dtype)\n",
        "      attention_scores = tf.expand_dims(tf.nn.softmax(score, axis=2), -1)\n",
        "      context = tf.reduce_sum(attention_scores * tf.expand_dims(keys, axis=1), axis=2)\n",
        "      return context, attention_scores,\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IDymTqFgpf9"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.english_embedding = Embedding(output_vocab_size, embedding_dim)\n",
        "    self.gru = GRU(dec_units, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(output_vocab_size)\n",
        "    self.attention = BahdanauAttention(dec_units)\n",
        "\n",
        "  def call(self, input_word_indices, encoder_keys, mask, state=None):\n",
        "    embedding_ = self.english_embedding(input_word_indices)\n",
        "    output, state = self.gru(embedding_, initial_state=state)\n",
        "    context, attention_scores = self.attention(query=output,\n",
        "                                              keys=encoder_keys,\n",
        "                                              mask = mask)\n",
        "    concat = tf.concat([output, context], axis=-1)\n",
        "\n",
        "\n",
        "    vocab_output = self.dense(concat)\n",
        "\n",
        "    return vocab_output, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm67dGzFgvN1"
      },
      "source": [
        "class ModelTrain(tf.keras.Model):\n",
        "  def __init__(self, input_text_processor, output_text_processor, embedding_dim, units):\n",
        "    super(ModelTrain, self).__init__()\n",
        "    self.encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\n",
        "    self.decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, data):\n",
        "    input_sentence, output_sentence = data\n",
        "    input_word_indices = self.input_text_processor(input_sentence)\n",
        "    output_word_indices = self.output_text_processor(output_sentence)\n",
        "    output_mask = tf.cast(output_word_indices != 0, tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      whole_encoder_states, final_hidden_state = self.encoder(input_word_indices)\n",
        "      vocab_output, decoder_last_state = self.decoder(input_word_indices=output_word_indices,\n",
        "                                                      encoder_keys = whole_encoder_states,\n",
        "                                                      mask = (input_word_indices != 0),\n",
        "                                                      state=final_hidden_state )\n",
        "\n",
        "      loss = tf.reduce_sum(self.loss(y_true=output_word_indices[:, 1:], y_pred=vocab_output[:, :-1]) * output_mask[:, 1:])\n",
        "    variables = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return {'loss': loss}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFIZOcpVgziy"
      },
      "source": [
        "model = ModelTrain(input_text_processor, output_text_processor, 300, 128)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "907VYxikg1LZ",
        "outputId": "57d82fb8-aad0-40d6-df2a-5abfdc91d5fa"
      },
      "source": [
        "model.fit(dataset, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1859/1859 [==============================] - 26s 12ms/step - loss: 1083.8786\n",
            "Epoch 2/10\n",
            "1859/1859 [==============================] - 22s 12ms/step - loss: 694.3065\n",
            "Epoch 3/10\n",
            "1859/1859 [==============================] - 22s 12ms/step - loss: 624.9959\n",
            "Epoch 4/10\n",
            "1859/1859 [==============================] - 22s 12ms/step - loss: 585.1982\n",
            "Epoch 5/10\n",
            "1859/1859 [==============================] - 22s 12ms/step - loss: 556.9671\n",
            "Epoch 6/10\n",
            "1859/1859 [==============================] - 21s 11ms/step - loss: 535.1006\n",
            "Epoch 7/10\n",
            "1859/1859 [==============================] - 21s 11ms/step - loss: 517.7254\n",
            "Epoch 8/10\n",
            "1859/1859 [==============================] - 21s 11ms/step - loss: 503.0345\n",
            "Epoch 9/10\n",
            "1859/1859 [==============================] - 21s 11ms/step - loss: 490.5161\n",
            "Epoch 10/10\n",
            "1859/1859 [==============================] - 21s 11ms/step - loss: 479.4623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9b00256410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0O0GCS0g3ba"
      },
      "source": [
        "class Infer(tf.Module):\n",
        "  def __init__(self,encoder, decoder,\n",
        "               input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.idx_to_word = (\n",
        "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "      vocabulary=output_text_processor.get_vocabulary())\n",
        "\n",
        "    token_mask_ids = index_from_string(['',\n",
        "                                    '[UNK]',\n",
        "                                    '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "    self.start_token = index_from_string('[START]')\n",
        "    self.end_token = index_from_string('[END]')\n",
        "\n",
        "  def tokens_to_text(self, result_tokens):\n",
        "    result_text_tokens = self.idx_to_word(result_tokens)\n",
        "\n",
        "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                        axis=1, separator=' ')\n",
        "\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    return result_text\n",
        "\n",
        "  def sample(self, logits):\n",
        "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "    return tf.argmax(logits, axis=-1)\n",
        "\n",
        "  def translate(self, input_sentence, max_length=50,):\n",
        "    batch_size = tf.shape(input_sentence)[0]\n",
        "    input_word_indices = self.input_text_processor(input_sentence)\n",
        "    input_mask = (input_word_indices != 0)\n",
        "    whole_encoder_output, final_hidden_state = self.encoder(input_word_indices)\n",
        "    dec_state = final_hidden_state\n",
        "    vocab_input = tf.fill([batch_size, 1], self.start_token)\n",
        "    result_tokens = []\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "    for current_length in range(max_length):\n",
        "      vocab_output, dec_state = self.decoder(input_word_indices=vocab_input,\n",
        "                                             encoder_keys=whole_encoder_output,\n",
        "                                             mask= input_mask, state= dec_state)\n",
        "      vocab_input = self.sample(vocab_output)\n",
        "\n",
        "\n",
        "      done = done | (vocab_input == self.end_token)\n",
        "      vocab_input = tf.where(done, tf.constant(0, dtype=tf.int64), vocab_input)\n",
        "      result_tokens.append(vocab_input)\n",
        "\n",
        "      if tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.tokens_to_text(result_tokens)\n",
        "    return {'text': result_text}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MALrtPLHhHfN"
      },
      "source": [
        "infer = Infer(model.encoder, model.decoder, input_text_processor, output_text_processor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaPjhwG8hJGL",
        "outputId": "51f7750f-06fc-4d4d-9ffe-1283055b9581"
      },
      "source": [
        "infer.translate([\"Te quiero.\", \"mi pasaporte esta aqui\", \"¿Dónde está el hotel?\", \"Quiero irme a casa hoy.\", \"La tierra gira alrededor del sol.\"])['text'].numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'i love you .', b'my wife is here here .', b'where is the bus ?',\n",
              "       b'i want to go home today .',\n",
              "       b'the girl was late for the of the bus .'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x6_-9o7hKyz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, time, random\n",
        "import h5py\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PGgUlDOdgAZO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.utils import *\n",
        "from keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "-I1WQI-KgDij"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = \"fra.txt\""
      ],
      "metadata": {
        "id": "mJzkaeclgD-k"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ],
      "metadata": {
        "id": "rZnHVeuVgHxE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "rzVzP6iYgJcU",
        "outputId": "7a096734-4e3d-48f4-838e-39db0fd95bfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 71\n",
            "Number of unique output tokens: 94\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
      ],
      "metadata": {
        "id": "TJrj8nrTgLhs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "TOgXqhojgOXT"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "# print(model.layers[-1].input)\n",
        "print(model.layers[-3].output)"
      ],
      "metadata": {
        "id": "P5R5_Gb9gQzF",
        "outputId": "10eb0a74-99b5-4d6c-bb1b-4ea1ba28371f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, None, 71)]   0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, None, 94)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, 256),        335872      ['input_7[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)                  [(None, None, 256),  359424      ['input_8[0][0]',                \n",
            "                                 (None, 256),                     'lstm_4[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_4[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, None, 94)     24158       ['lstm_5[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 719,454\n",
            "Trainable params: 719,454\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_4')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_4')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_4')>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"encoder_input_data shape:\",encoder_input_data.shape)\n",
        "print(\"decoder_input_data shape:\",decoder_input_data.shape)\n",
        "print(\"decoder_target_data shape:\",decoder_target_data.shape)"
      ],
      "metadata": {
        "id": "KlzMYAWagSVE",
        "outputId": "8a588cfb-c327-496a-f330-282fd0a3313c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_input_data shape: (10000, 16, 71)\n",
            "decoder_input_data shape: (10000, 59, 94)\n",
            "decoder_target_data shape: (10000, 59, 94)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model,show_shapes=True)"
      ],
      "metadata": {
        "id": "JMwD8yAVgVqU",
        "outputId": "48d882e1-6d77-4070-9f2e-45dc25e17292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "from keras.optimizers import *\n",
        "model.compile(optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.001), loss='categorical_crossentropy')\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "id": "Kk8IPOqagXek",
        "outputId": "af381619-8643-4baa-814b-5513227b03bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "125/125 [==============================] - 6s 21ms/step - loss: 0.9629 - val_loss: 1.0740\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.8817 - val_loss: 1.0396\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.8365 - val_loss: 0.9684\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.7898 - val_loss: 0.9264\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.7506 - val_loss: 0.8800\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.6999 - val_loss: 0.8348\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6579 - val_loss: 0.7820\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6393 - val_loss: 0.7888\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6046 - val_loss: 0.7198\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.5704 - val_loss: 0.6963\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.5421 - val_loss: 0.6697\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.5188 - val_loss: 0.6593\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.4983 - val_loss: 0.6297\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4816 - val_loss: 0.6345\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.4684 - val_loss: 0.6138\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.4568 - val_loss: 0.6083\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.4456 - val_loss: 0.5947\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.4377 - val_loss: 0.5951\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.4291 - val_loss: 0.5854\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4229 - val_loss: 0.5840\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.4173 - val_loss: 0.5811\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.4089 - val_loss: 0.5783\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.4023 - val_loss: 0.5758\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3992 - val_loss: 0.5744\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3916 - val_loss: 0.5704\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3853 - val_loss: 0.5676\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.3795 - val_loss: 0.5635\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.3747 - val_loss: 0.5659\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.5629\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3652 - val_loss: 0.5622\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3615 - val_loss: 0.5626\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3568 - val_loss: 0.5648\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3554 - val_loss: 0.5592\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3487 - val_loss: 0.5610\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.3435 - val_loss: 0.5637\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.3384 - val_loss: 0.5633\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3367 - val_loss: 0.5643\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3334 - val_loss: 0.5620\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3323 - val_loss: 0.5608\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.3263 - val_loss: 0.5607\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3221 - val_loss: 0.5608\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3187 - val_loss: 0.5631\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.3146 - val_loss: 0.5599\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.3112 - val_loss: 0.5640\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.3090 - val_loss: 0.5668\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3059 - val_loss: 0.5778\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.3075 - val_loss: 0.5632\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.3028 - val_loss: 0.5656\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3001 - val_loss: 0.5643\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2955 - val_loss: 0.5657\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.2921 - val_loss: 0.5692\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.2893 - val_loss: 0.5685\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2878 - val_loss: 0.5692\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2866 - val_loss: 0.5707\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2829 - val_loss: 0.5718\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2803 - val_loss: 0.5712\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2785 - val_loss: 0.5724\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2790 - val_loss: 0.5745\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2747 - val_loss: 0.5769\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.2725 - val_loss: 0.5742\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2706 - val_loss: 0.5765\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2687 - val_loss: 0.5766\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2674 - val_loss: 0.5774\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.2653 - val_loss: 0.5791\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.2634 - val_loss: 0.5787\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2630 - val_loss: 0.5818\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.2637 - val_loss: 0.5776\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.2606 - val_loss: 0.5816\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2570 - val_loss: 0.5803\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2556 - val_loss: 0.5824\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2543 - val_loss: 0.5838\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2520 - val_loss: 0.5843\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2506 - val_loss: 0.5830\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2494 - val_loss: 0.5832\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.2473 - val_loss: 0.5855\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2469 - val_loss: 0.5867\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2453 - val_loss: 0.5901\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2439 - val_loss: 0.5867\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2440 - val_loss: 0.5892\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2424 - val_loss: 0.5891\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2402 - val_loss: 0.5910\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2384 - val_loss: 0.5923\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2389 - val_loss: 0.5924\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.2383 - val_loss: 0.5934\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2360 - val_loss: 0.5920\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2342 - val_loss: 0.5933\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2331 - val_loss: 0.5952\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2317 - val_loss: 0.5966\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2335 - val_loss: 0.5969\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2316 - val_loss: 0.5963\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.2300 - val_loss: 0.5979\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.2283 - val_loss: 0.5959\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2294 - val_loss: 0.6025\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2312 - val_loss: 0.5992\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2271 - val_loss: 0.5998\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2245 - val_loss: 0.6017\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2240 - val_loss: 0.6007\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2224 - val_loss: 0.6005\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2219 - val_loss: 0.6014\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.2212 - val_loss: 0.6022\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa20eb31f00>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "C1beVfBMgZSU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "Cp1dVe5ugdkc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "metadata": {
        "id": "zqq3DRJTgfcE",
        "outputId": "dc9d94a5-509d-46a6-c90b-354f098ee0ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 341ms/step\n",
            "1/1 [==============================] - 0s 390ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Cons mai.\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Courez !\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Courez !\n",
            "\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Quelle prou.\n",
            "\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Au feu.\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Monter.\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Sarte.\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrête-toi.\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrête-toi.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrête-toi.\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attende !\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attende !\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Je comprens.\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: J'essaye.\n",
            "\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Je l'ai emporté.\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Je l'ai emporté.\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "-\n",
            "Input sentence: Oh no!\n",
            "Decoded sentence: Sante.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: Attack!\n",
            "Decoded sentence: Attaque !\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: Attack!\n",
            "Decoded sentence: Attaque !\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Sante.\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Sante.\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Sante.\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Sante.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Vour.\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Vas-y mairtir.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Vas-y mairtir.\n",
            "\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Vas-y mairtir.\n",
            "\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "-\n",
            "Input sentence: Got it!\n",
            "Decoded sentence: Parte.\n",
            "\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "-\n",
            "Input sentence: Got it!\n",
            "Decoded sentence: Parte.\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris.\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris.\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Hop in.\n",
            "Decoded sentence: Monte.\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "-\n",
            "Input sentence: Hop in.\n",
            "Decoded sentence: Monte.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "-\n",
            "Input sentence: Hug me.\n",
            "Decoded sentence: Serrez-moi confair !\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Hug me.\n",
            "Decoded sentence: Serrez-moi confair !\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: I fell.\n",
            "Decoded sentence: Je suis tombé.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "-\n",
            "Input sentence: I fell.\n",
            "Decoded sentence: Je suis tombé.\n",
            "\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "-\n",
            "Input sentence: I know.\n",
            "Decoded sentence: Je sais.\n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: I left.\n",
            "Decoded sentence: Je suis parti.\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: I left.\n",
            "Decoded sentence: Je suis parti.\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "-\n",
            "Input sentence: I lost.\n",
            "Decoded sentence: J'ai perdu.\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: I'm 19.\n",
            "Decoded sentence: J'ai houre.\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: Je vais bien.\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: Je vais bien.\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "-\n",
            "Input sentence: Listen.\n",
            "Decoded sentence: Regarde !\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas de mais.\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vrai ?\n",
            "\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vrai ?\n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vrai ?\n",
            "\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: Thanks.\n",
            "Decoded sentence: Merci !\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: We try.\n",
            "Decoded sentence: Nous avons de tour.\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous l'avons estorés.\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous l'avons estorés.\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous l'avons estorés.\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous l'avons estorés.\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "-\n",
            "Input sentence: Ask Tom.\n",
            "Decoded sentence: Demande à Tom.\n",
            "\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: Awesome!\n",
            "Decoded sentence: Sont soctis le mais.\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calmes.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calmes.\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calmes.\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-\n",
            "Input sentence: Be cool.\n",
            "Decoded sentence: Sois gagne.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez étais ?\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez étais ?\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez étais ?\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez étais ?\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez étais ?\n",
            "\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez étais ?\n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "-\n",
            "Input sentence: Be kind.\n",
            "Decoded sentence: Sois gentil.\n",
            "\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille.\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille.\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille.\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille.\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille.\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille.\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "-\n",
            "Input sentence: Beat it.\n",
            "Decoded sentence: Soyez gagnes.\n",
            "\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "-\n",
            "Input sentence: Call me.\n",
            "Decoded sentence: Appellez-moi.\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: Call me.\n",
            "Decoded sentence: Appellez-moi.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "-\n",
            "Input sentence: Call us.\n",
            "Decoded sentence: Appelle-nous.\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "-\n",
            "Input sentence: Call us.\n",
            "Decoded sentence: Appelle-nous.\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "-\n",
            "Input sentence: Come in.\n",
            "Decoded sentence: Entre.\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "-\n",
            "Input sentence: Come in.\n",
            "Decoded sentence: Entre.\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "-\n",
            "Input sentence: Come in.\n",
            "Decoded sentence: Entre.\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "-\n",
            "Input sentence: Come in.\n",
            "Decoded sentence: Entre.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "-\n",
            "Input sentence: Come on!\n",
            "Decoded sentence: Entrer.\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "-\n",
            "Input sentence: Come on.\n",
            "Decoded sentence: Venez !\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "-\n",
            "Input sentence: Come on.\n",
            "Decoded sentence: Venez !\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "-\n",
            "Input sentence: Come on.\n",
            "Decoded sentence: Venez !\n",
            "\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "-\n",
            "Input sentence: Drop it!\n",
            "Decoded sentence: Laissez l'avorter.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o_p_HVtNghSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}